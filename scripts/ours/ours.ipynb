{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import falcor\n",
    "import time\n",
    "import numpy as np\n",
    "import pyexr as exr\n",
    "import sys\n",
    "import os\n",
    "import dataclasses\n",
    "import datetime\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), \"..\"))\n",
    "import common\n",
    "import material_utils\n",
    "from loss import compute_render_loss_L1, compute_render_loss_L2\n",
    "\n",
    "\n",
    "import common\n",
    "import os\n",
    "from falcor import Camera, float3, uint2\n",
    "import copy\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "try:\n",
    "\timport tinycudann as tcnn\n",
    "except ImportError:\n",
    "\tprint(\"This sample requires the tiny-cuda-nn extension for PyTorch.\")\n",
    "\tprint(\"You can install it by running:\")\n",
    "\tprint(\"============================================================\")\n",
    "\tprint(\"tiny-cuda-nn$ cd bindings/torch\")\n",
    "\tprint(\"tiny-cuda-nn/bindings/torch$ python setup.py install\")\n",
    "\tprint(\"============================================================\")\n",
    "\tsys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "class ExperimentParams:\n",
    "    def __init__(self):\n",
    "        self.MODELS_PATH = \"models/\"\n",
    "\n",
    "params = ExperimentParams()\n",
    "print(params.MODELS_PATH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def float3tonumpy(v):\n",
    "    return np.array([v.x, v.y, v.z])\n",
    "\n",
    "def numpytofloat3(n):\n",
    "    return float3(*np.copy(n))\n",
    "\n",
    "def numpytouint2(n):\n",
    "    return uint2(*np.copy(n))\n",
    "\n",
    "\n",
    "class ExperimentParams:\n",
    "    #render_width = 256 # random\n",
    "    #render_height = 256\n",
    "    render_width = 256 # random\n",
    "    render_height = 256\n",
    "\n",
    "    enable_MIS = False \n",
    "    enable_sky_learning = True # for example we could disable it and compare NIRC with NRC\n",
    "\n",
    "    relative_error = False\n",
    "\n",
    "    USE_GBUFFER = True\n",
    "    relative_error_eps = 0.001\n",
    "    \n",
    "\n",
    "    RECOMPUTE_LOSS = {\"NIRC\": False, \"NIRC_EM\": False, \"SH\": False, \"VMF\": False, \"NCV\": False}\n",
    "\n",
    "    GRADIENT_CLIP = 0.001\n",
    "    SKIP_TENSOR_CHECK = True # disable if you have problems with NaNs, Inf or just 0 loss\n",
    "    ADAM_BETAS = [0.98, 0.9999]\n",
    "    ADAM_LR = 0.01\n",
    "    num_training_frames = 150 # we gotta use the same number of training frames for all models to make conduct fair experiments!. USE a BIIG number for the final experiments (4k? 5k?)\n",
    "\n",
    "    # MODELS_PATH = \"E:/Models/\" # we can just put it in the same folder as the script?\n",
    "    MODELS_PATH = params.MODELS_PATH\n",
    "\n",
    "    is_training = False\n",
    "\n",
    "\n",
    "\n",
    "gparams = ExperimentParams()\n",
    "\n",
    "\n",
    "\n",
    "class ValidationPoint:\n",
    "    xn: float\n",
    "    yn: float\n",
    "\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "    id: int\n",
    "    \n",
    "    position: np.ndarray\n",
    "    target: np.ndarray\n",
    "    normal: np.ndarray\n",
    "\n",
    "    is_init: bool\n",
    "\n",
    "    def __init__(self, xn: float, yn: float):\n",
    "        self.xn = xn\n",
    "        self.yn = yn\n",
    "\n",
    "        self.x = int(gparams.render_width*xn)\n",
    "        self.y = int(gparams.render_height*yn)\n",
    "\n",
    "        self.id = self.y*gparams.render_width+self.x\n",
    "\n",
    "        self.is_init = False\n",
    "\n",
    "\n",
    "    def init_surface_data(self, p: np.ndarray, t: np.ndarray, n: np.ndarray):\n",
    "        self.is_init = True\n",
    "\n",
    "        self.position = p\n",
    "        self.target = t\n",
    "        self.n = n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SceneConfig:\n",
    "    model_path: str\n",
    "    camera_position: np.ndarray\n",
    "    camera_target: np.ndarray\n",
    "    tonemapper_exposure: float\n",
    "    focal_length: float\n",
    "    \n",
    "    validation_points: List[ValidationPoint]\n",
    "    emissive_factor: float = 1.0\n",
    "    \n",
    "\n",
    "    def __init__(self, model_path: str, camera_position: Tuple[float, float, float], camera_target: Tuple[float, float, float], tonemapper_exposure: float = 0, focal_length: float = None, selected_points: List[Tuple[float, float]] = [], \n",
    "                 up: Tuple[float, float, float] = [0.0, 1.0, 0.0], lr_factor: float = 1.0, epochs: int = 4000, roughness: float = 1.0, var_est_steps: int = 1000):\n",
    "        self.model_path = gparams.MODELS_PATH + model_path\n",
    "        self.model_name = os.path.dirname(model_path)\n",
    "        self.camera_position = np.array(camera_position)\n",
    "        self.camera_target = np.array(camera_target)\n",
    "        self.epochs = epochs\n",
    "        self.var_est_steps = var_est_steps\n",
    "        self.tonemapper_exposure = tonemapper_exposure\n",
    "        self.roughness = roughness\n",
    "        self.lr_factor = lr_factor\n",
    "        self.checkpoint_dir = os.path.join('checkpoints', self.model_name)\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        \n",
    "        self.focal_length = focal_length\n",
    "        self.validation_points = []\n",
    "        self.up = up\n",
    "\n",
    "        for p in selected_points:\n",
    "            self.validation_points.append(ValidationPoint(xn=p[0], yn=p[1]))\n",
    "\n",
    "    def camera_position_f3(self) -> float3:\n",
    "        return numpytofloat3(self.camera_position)\n",
    "\n",
    "    def camera_target_f3(self) -> float3:\n",
    "        return numpytofloat3(self.camera_target)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"SceneConfig(model_path={self.model_path}, camera_position={self.camera_position.tolist()}, camera_target={self.camera_target.tolist()}, tonemapper_exposure={self.tonemapper_exposure}, focal_length={self.focal_length})\"\n",
    "\n",
    "    def add_validation_point(self, p: Tuple[float, float]):\n",
    "        self.validation_points.append(ValidationPoint(xn=p[0], yn=p[1]))\n",
    "\n",
    "    def setup_validation_points(self, mlData):\n",
    "        positions = mlData[\"worldpos\"] \n",
    "        normals = mlData[\"normal\"]\n",
    "\n",
    "        for i in range(len(self.validation_points)):\n",
    "            p = self.validation_points[i]\n",
    "            pos = positions[p.id].cpu().numpy() # we should get it from the gpu torch in some way \n",
    "            normal = normals[p.id].cpu().numpy()\n",
    "\n",
    "            p.init_surface_data(p = pos+normal*0.00001, t = pos+normal+normal*0.00001, n = normal)\n",
    "            \n",
    "    def prepare_scene(self, scene, debugPointID=None):\n",
    "        scene.roughnessMultiplier = self.roughness\n",
    "\n",
    "        if debugPointID == None:\n",
    "            scene.camera.position = numpytofloat3(self.camera_position)\n",
    "            scene.camera.target = numpytofloat3(self.camera_target)\n",
    "            if self.focal_length != None:\n",
    "                scene.camera.focalLength = self.focal_length\n",
    "            scene.camera.useHemisphericalCamera = False\n",
    "            scene.camera.up = numpytofloat3(self.up)\n",
    "        else:\n",
    "            vp = self.validation_points[debugPointID]\n",
    "            scene.camera.useHemisphericalCamera = True\n",
    "            scene.camera.position = numpytofloat3(vp.position)\n",
    "            scene.camera.target = numpytofloat3(vp.target)\n",
    "            scene.camera.up = numpytofloat3(self.up)\n",
    "            assert(vp.is_init)\n",
    "\n",
    "\n",
    "scenes = {\n",
    "    \"CornellBox\": SceneConfig(\"CornellBox/cornell_box.pyscene\", camera_position=[0, 0.28, 0.6], camera_target=[0, 0.28, 0], selected_points=[[0.35, 0.5], [0.65, 0.4]], epochs=2000),\n",
    "    #\"CornellBoxEnv\": SceneConfig(\"CornellBoxEnv/cornell_box_env.pyscene\", camera_position=[0, 0.28, 0.6], camera_target=[0, 0.28, 0], selected_points=[[0.35, 0.5], [25.0/256.0, 150.0/256.0]]),\n",
    "    #\"EnvDebug\": SceneConfig(\"EnvDebug/env_debug.pyscene\", camera_position=[0, 0.28, 0.6], camera_target=[0, 0.28, 0], selected_points=[[0.35, 0.5], [0.65, 0.4]]),\n",
    "    #\"CornellBox\": SceneConfig(\"CornellBox/cornell_box.pyscene\", camera_position=[0, 0.28, 1.2], camera_target=[0, 0.28, 0], selected_points=[[0.35, 0.5], [0.65, 0.4]]),\n",
    "    #\"Bistro\" : SceneConfig(\"Bistro/BistroExterior.pyscene\", camera_position=[-29.195, 5.145, -8.768], camera_target=[-28.212, 5.137, -8.586], selected_points=[[400.0/1280, 400.0/720], [410/1200.0, 200.0/720], [800/1280.0, 600/720.0]]),\n",
    "    #\"CountryKitchen\": SceneConfig(\"CountryKitchen/Country-Kitchen.gltf\", camera_position=[1.456, 1.509, 1.602], camera_target=[0.678, 1.471, 0.974], selected_points=[[0.35, 0.5], [0.65, 0.4]], lr_factor=0.5, epochs=2000, var_est_steps=3000),\n",
    "    #\"SanMiguel\": SceneConfig(\"SanMiguel/san-miguel.pyscene\", camera_position=[22.2190, 3.3300, 6.7120], camera_target=[21.5799, 3.3054, 5.9432], selected_points=[[0.35, 0.5], [0.65, 0.4]], focal_length=17.750),\n",
    "    #\"TheWhiteRoomCycles\": SceneConfig(\"TheWhiteRoomCycles/the-white-room_0001.gltf\", camera_position=[2.781, 1.247, 5.251], camera_target=[2.151, 1.195, 4.477], selected_points=[[0.35, 0.5], [0.65, 0.4]], lr_factor=0.25),\n",
    "    #\"Sponza\": SceneConfig(\"Sponza/Sponza.pyscene\", camera_position=[8.084, 1.708, 0.827], camera_target=[7.091, 1.684, 0.715], selected_points=[[0.5, 0.1], [0.65, 0.2]], focal_length=16.750),\n",
    "    #\"SponzaSpecular\": SceneConfig(\"SponzaSpecular/SponzaSpecular.pyscene\", camera_position=[-9.0311, 1.1590, -1.2149], camera_target=[-8.0785, 1.0369, -0.9365], selected_points=[[0.5, 0.8], [0.15, 0.5]], epochs=2000, roughness=0.25) \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_cfg = scenes[\"CornellBox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DIR = os.path.abspath('')\n",
    "sys.path.append(os.path.join(CUR_DIR, \"..\"))\n",
    "\n",
    "\n",
    "output_dir = CUR_DIR + \"/results/\"\n",
    "\n",
    "\n",
    "device_id = 0\n",
    "testbed = common.create_testbed([gparams.render_width, gparams.render_height])\n",
    "device = testbed.device\n",
    "\n",
    "\n",
    "# Load the reference scene.\n",
    "ref_scene = common.load_scene(\n",
    "    testbed,\n",
    "    scene_cfg.model_path,\n",
    "    gparams.render_width / gparams.render_height,\n",
    ")\n",
    "\n",
    "\n",
    "# I don't know why but useAnalyticLights and useEnvLight have False value if we start the renderer from the python side. haven't managed to found the root of this bizarre behaviour. it must be a bug -> TO DO: MAKE SURE and REPORT ABOUT IT!!\n",
    "ref_scene.renderSettings = falcor.SceneRenderSettings(useEnvLight=True, useAnalyticLights=True, useEmissiveLights=True, useGridVolumes=True)\n",
    "\n",
    "\n",
    "# init structure buffers that we need for ML side\n",
    "# color = albedo+specular reflectance\n",
    "field_types = {\"radiance\": \"float3\", \"dir\": \"float3\", \"thp\": \"float3\", \"worldpos\": \"float3\", \"normal\": \"float3\", \"color\": \"float3\", \"dradiance\": \"float3\", \"view\": \"float3\", \"roughness\": \"float\", \"pdf\": \"float\"}\n",
    "ml_data = device.create_structured_buffer(\n",
    "    struct_size = 12*8+4*2,\n",
    "    element_count=gparams.render_width*gparams.render_height,\n",
    "    bind_flags=falcor.ResourceBindFlags.ShaderResource  \n",
    "    | falcor.ResourceBindFlags.UnorderedAccess\n",
    "    | falcor.ResourceBindFlags.Shared\n",
    ")\n",
    "\n",
    "\n",
    "rays_fields = {\"worldpos\": \"float3\", \"dir\": \"float3\"}\n",
    "ml_rays_data = device.create_structured_buffer(\n",
    "    struct_size = 12*2,\n",
    "    element_count=gparams.render_width*gparams.render_height,\n",
    "    bind_flags=falcor.ResourceBindFlags.ShaderResource  \n",
    "    | falcor.ResourceBindFlags.UnorderedAccess\n",
    "    | falcor.ResourceBindFlags.Shared\n",
    ")\n",
    "\n",
    "\n",
    "device.render_context.wait_for_falcor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_graph = testbed.create_render_graph(\"StandardPathTracer\")\n",
    "\n",
    "# Create the PathTracer pass.\n",
    "path_tracer_pass = render_graph.create_pass(\n",
    "    \"PathTracer\",\n",
    "    \"PathTracer\",\n",
    "    {\n",
    "        \"samplesPerPixel\": 1, \"useSER\": False, \"useMIS\": gparams.enable_MIS, \"disableCaustics\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "primary_render_pass_name = \"GBufferRT\" if gparams.USE_GBUFFER else \"VBufferRT\"\n",
    "if  gparams.USE_GBUFFER:\n",
    "    primary_render_pass = render_graph.create_pass(\n",
    "        primary_render_pass_name,\n",
    "        primary_render_pass_name,\n",
    "        {\n",
    "            \"samplePattern\": \"Center\",\n",
    "            \"sampleCount\": 1,\n",
    "            \"useAlphaTest\": True\n",
    "        }\n",
    "    )\n",
    "    render_graph.mark_output(primary_render_pass_name+\".vbuffercache\")\n",
    "    render_graph.mark_output(primary_render_pass_name+\".brdf\")\n",
    "else:\n",
    "    # Create the VBufferRT pass.\n",
    "    primary_render_pass = render_graph.create_pass(\n",
    "        primary_render_pass_name,\n",
    "        primary_render_pass_name,\n",
    "        {\n",
    "            \"samplePattern\": \"Center\",\n",
    "            \"sampleCount\": 1,\n",
    "            \"useAlphaTest\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "AccumulatePass = render_graph.createPass(\"AccumulatePass\", \"AccumulatePass\", {'enabled': True, 'precisionMode': 'Single'})\n",
    "ToneMapper = render_graph.createPass(\"ToneMapper\", \"ToneMapper\", {'autoExposure': False, 'exposureCompensation': 0.0, 'outputFormat': 'RGBA32Float' }) \n",
    "\n",
    "\n",
    "\n",
    "# Add edges to connect the passes.\n",
    "\n",
    "render_graph.add_edge(primary_render_pass_name+\".vbuffer\", \"PathTracer.vbuffer\")\n",
    "\n",
    "\n",
    "render_graph.add_edge(primary_render_pass_name+\".viewW\", \"PathTracer.viewW\")\n",
    "render_graph.add_edge(primary_render_pass_name+\".mvec\", \"PathTracer.mvec\")\n",
    "\n",
    "render_graph.addEdge(\"PathTracer.color\", \"AccumulatePass.input\")\n",
    "render_graph.addEdge(\"AccumulatePass.output\", \"ToneMapper.src\")\n",
    "\n",
    "\n",
    "# Mark the output of the PathTracer pass.\n",
    "render_graph.markOutput(\"ToneMapper.dst\")\n",
    "render_graph.mark_output(\"AccumulatePass.output\")\n",
    "render_graph.mark_output(\"PathTracer.color\")\n",
    "\n",
    "# Assign the configured render graph to the testbed.\n",
    "testbed.render_graph = render_graph\n",
    "\n",
    "path_tracer_pass.mlData = ml_data\n",
    "primary_render_pass.mlRaysData = ml_rays_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_cfg.prepare_scene(ref_scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def adjust_gamma(image, gamma=2.2):\n",
    "\treturn (image )**(1 / gamma)\n",
    "\n",
    "\n",
    "def frameRender(num_samples=1024, vis = True, tonemapped=False,  directEmissive=True, directSky=True):\n",
    "    # may take some time to recompile the shaders becase of changed defines\n",
    "    AccumulatePass.reset()\n",
    "    AccumulatePass.enabled = True\n",
    "    primary_render_pass.sampleCount = 16\n",
    "    \n",
    "    path_tracer_pass.useMIS = True\n",
    "    path_tracer_pass.mlTraining = False\n",
    "    path_tracer_pass.directEmissive = directEmissive\n",
    "    path_tracer_pass.directSky = directSky\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        testbed.frame()\n",
    "\n",
    "    if tonemapped:\n",
    "        img = testbed.render_graph.get_output(\"ToneMapper.dst\").to_numpy()[:, :, :3]\n",
    "        img = adjust_gamma(img)\n",
    "    else:\n",
    "        img = testbed.render_graph.get_output(\"AccumulatePass.output\").to_numpy()[:, :, :3]\n",
    "    if not vis:\n",
    "        return img\n",
    "\n",
    "\n",
    "    plt.imshow(img)\n",
    "\n",
    "    path_tracer_pass.directEmissive = True\n",
    "    path_tracer_pass.directSky = True\n",
    "    path_tracer_pass.useMIS = gparams.enable_MIS \n",
    "    primary_render_pass.sampleCount = 1\n",
    "    AccumulatePass.enabled = False\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFrame():\n",
    "    path_tracer_pass.useMIS = False\n",
    "    path_tracer_pass.indirectSky = gparams.enable_sky_learning\n",
    "    path_tracer_pass.mlTraining = True\n",
    "    testbed.frame()\n",
    "    path_tracer_pass.useMIS = True\n",
    "    path_tracer_pass.indirectSky = True\n",
    "    path_tracer_pass.mlTraining = False\n",
    "        \n",
    "def setupBRDFCache():\n",
    "    path_tracer_pass.useMIS = False\n",
    "    path_tracer_pass.indirectSky = gparams.enable_sky_learning\n",
    "    path_tracer_pass.mlTraining = True\n",
    "    primary_render_pass.cacheVisibility = True\n",
    "    primary_render_pass.brdfRender = False\n",
    "    testbed.frame()\n",
    "    primary_render_pass.brdfRender = False\n",
    "    primary_render_pass.cacheVisibility = False\n",
    "    path_tracer_pass.useMIS = True\n",
    "    path_tracer_pass.indirectSky = True\n",
    "    path_tracer_pass.mlTraining = False\n",
    "\n",
    "\n",
    "def getBRDF(pixel):\n",
    "    path_tracer_pass.useMIS = False\n",
    "    path_tracer_pass.indirectSky = gparams.enable_sky_learning\n",
    "    path_tracer_pass.mlTraining = True\n",
    "    primary_render_pass.brdfRender = True\n",
    "    primary_render_pass.targetPixel = numpytouint2(pixel)\n",
    "    testbed.frame()\n",
    "    primary_render_pass.brdfRender = False\n",
    "    path_tracer_pass.useMIS = True\n",
    "    path_tracer_pass.indirectSky = True\n",
    "    path_tracer_pass.mlTraining = False\n",
    "    brdf = testbed.render_graph.get_output(primary_render_pass_name+\".brdf\").to_numpy()[:, :, :3]\n",
    "    return brdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testbed.end_frame_forced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to remove it\n",
    "scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "im = frameRender(num_samples = 1, vis=True, tonemapped=True)\n",
    "# this should produce a converged render from the correct camera viewpoint (+- as in the paper teaser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def falcor_to_torch(buffer: falcor.Buffer, dtype=torch.float32):\n",
    "    params = torch.tensor([0]*(buffer.element_count*12), dtype=dtype)\n",
    "    buffer.copy_to_torch(params)\n",
    "    device.render_context.wait_for_cuda()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_field(data, start, struct_size, field_size):\n",
    "    return torch.cat([data[i:i + field_size] for i in range(start, data.numel(), struct_size)])\n",
    "\n",
    "def falcor_to_torch_split_interleaved(buffer, field_types, dtype=torch.float32):\n",
    "    # Size mapping for different types (add more types if needed)\n",
    "    size_mapping = {\n",
    "        \"float3\": 3,  # 3 floats in a float3\n",
    "        \"float\": 1\n",
    "    }\n",
    "\n",
    "    # Calculate the size of one complete set of fields\n",
    "    struct_size = sum(size_mapping[field_type] for field_type in field_types.values())\n",
    "    # Calculate the total size of the tensor\n",
    "    total_size = struct_size * buffer.element_count    \n",
    "    all_data = buffer.to_torch([total_size])\n",
    "    device.render_context.wait_for_cuda()\n",
    "\n",
    "    # Splitting the tensor into separate tensors for each field considering interleaved structure\n",
    "    tensors = {}\n",
    "    offset = 0\n",
    "    for i, (field_name, field_type) in enumerate(field_types.items()):\n",
    "        field_size  = size_mapping[field_type]\n",
    "\n",
    "        # Reshaping data\n",
    "        reshaped_data = all_data.view(-1, struct_size)\n",
    "        # Extracting field without using a loop\n",
    "        tensors[field_name] = reshaped_data[:, offset: offset + field_size]\n",
    "        offset += field_size\n",
    "\n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "mlDataRaysOutput = falcor_to_torch_split_interleaved(ml_rays_data, rays_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlDataOutput_ref = {k: v.clone() for k, v in mlDataOutput.items()}\n",
    "mlDataRaysOutput_ref = {k: v.clone() for k, v in mlDataRaysOutput.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we gotta inform the validation points about the corresponding surface parameters (where we're gonna capute the hemispherical incident light visualization) \n",
    "#scene_cfg.add_validation_point([900/1280.0, 100/720.0])\n",
    "scene_cfg.setup_validation_points(mlDataOutput_ref)\n",
    "scene_cfg.prepare_scene(ref_scene, debugPointID=0)\n",
    "im = frameRender(num_samples = 1, vis=True, tonemapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "im = frameRender(num_samples = 1, vis=True, tonemapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_cfg.prepare_scene(ref_scene)\n",
    "_ = frameRender(num_samples=1, tonemapped=True)\n",
    "setupBRDFCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_tensor(tensor, tensor_name=\"Tensor\", force_analyze=False, min=None, max=None):\n",
    "    if gparams.SKIP_TENSOR_CHECK and not force_analyze:\n",
    "        return 1\n",
    "    \"\"\"\n",
    "    Check if the given tensor contains any NaN or Inf values and print a specific message.\n",
    "\n",
    "    Args:\n",
    "    tensor (torch.Tensor): The tensor to check.\n",
    "    tensor_name (str): The name of the tensor to display in messages.\n",
    "    \"\"\"\n",
    "    has_nan = torch.isnan(tensor).any().item()  # Check for any NaNs and convert to Python bool\n",
    "    has_inf = torch.isinf(tensor).any().item()  # Check for any Infs and convert to Python bool\n",
    "\n",
    "\n",
    "    good = True\n",
    "    # Print relevant messages based on the tensor's content\n",
    "    if has_nan and has_inf:\n",
    "        print(f\"{tensor_name} contains both NaN and Inf values.\")\n",
    "        good = False\n",
    "    elif has_nan:\n",
    "        print(f\"{tensor_name} contains NaN values.\")\n",
    "        good = False\n",
    "    elif has_inf:\n",
    "        print(f\"{tensor_name} contains Inf values.\")\n",
    "        good = False\n",
    "\n",
    "\n",
    "    \n",
    "    if min is not None and torch.all(tensor >= min).item() is False:\n",
    "        good = False\n",
    "        print(f\"{tensor_name} is smaller than {min}\")\n",
    "        \n",
    "    if max is not None and torch.all(tensor <= max).item() is False:\n",
    "        good = False\n",
    "        print(f\"{tensor_name} is bigger than {max}\")\n",
    "\n",
    "\n",
    "    if good and not force_analyze:\n",
    "        return True\n",
    "\n",
    "    # Checking for NaNs and Infs\n",
    "    has_nan = torch.isnan(tensor).any().item()\n",
    "    has_inf = torch.isinf(tensor).any().item()\n",
    "    print(\"Contains NaN values:\", has_nan)\n",
    "    print(\"Contains Inf values:\", has_inf)\n",
    "    \n",
    "    # Basic statistics\n",
    "    tensor_min = torch.min(tensor).item()\n",
    "    tensor_max = torch.max(tensor).item()\n",
    "    tensor_mean = torch.mean(tensor.float()).item()  # Ensure tensor is float for mean calculation\n",
    "    print(\"Minimum value:\", tensor_min)\n",
    "    print(\"Maximum value:\", tensor_max)\n",
    "    print(\"Average value:\", tensor_mean)\n",
    "    \n",
    "    # Histogram of the tensor values\n",
    "    tensor_np = tensor.detach().cpu().numpy()  # Convert tensor to NumPy array for histogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(tensor_np.ravel(), bins=100, color='blue', alpha=0.7)  # Flatten the tensor and plot\n",
    "    plt.title(f\"Histogram of {tensor_name} with shape {tensor.shape}\")\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mask = (mlDataOutput_ref[\"thp\"] > 0.0).all(dim=1)\n",
    "\n",
    "def get_mask_for_training(rad, default_mask_use=True):\n",
    "    active_pixel_mask = (rad >= 0).all(dim=1)\n",
    "    nan_mask = torch.isnan(rad).any(dim=1)  # Creates a mask of rows with any NaNs\n",
    "    inf_mask = torch.isinf(rad).any(dim=1)  # Creates a mask of rows with any NaNs\n",
    "    non_nan_mask = ~nan_mask\n",
    "    non_inf_mask = ~inf_mask\n",
    "    non_nan_mask = (non_nan_mask & active_pixel_mask) & non_inf_mask\n",
    "    if default_mask_use:\n",
    "        non_nan_mask = default_mask & non_nan_mask\n",
    "    return non_nan_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_t = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def packModelInput(dposition, ddir, dcolor, droughness, dnormal):\n",
    "\n",
    "    if ddir is not None:\n",
    "        norm = (torch.square(torch.sum(ddir**2, dim=1))+1.0e-10)\n",
    "        norm = torch.unsqueeze(norm, dim=1)\n",
    "        ddir = ddir/norm\n",
    "        dirs = ddir*0.5+0.5\n",
    "        \n",
    "    normals = cart_to_sph(dnormal)\n",
    "\n",
    "    # torch.Size([3])\n",
    "    scene_min = torch.tensor(float3tonumpy(ref_scene.bounds.min_point), device=\"cuda:0\")\n",
    "    scene_max = torch.tensor(float3tonumpy(ref_scene.bounds.max_point), device=\"cuda:0\")\n",
    "\n",
    "    #torch.Size([147600, 3])\n",
    "    positions = (dposition-scene_min)/(scene_max-scene_min)\n",
    "\n",
    "    if ddir is not None:\n",
    "        assert(check_tensor(dirs, \"dirs\"))\n",
    "\n",
    "    assert(check_tensor(normals, \"normals\"))\n",
    "    assert(check_tensor(positions, \"positions\"))\n",
    "    assert(check_tensor(dcolor, \"dcolor\"))\n",
    "    assert(check_tensor(droughness, \"droughness\"))\n",
    "\n",
    "    if ddir is not None:\n",
    "        return torch.cat((normals, positions, droughness, dcolor, dirs), dim=1)\n",
    "    else:\n",
    "        return torch.cat((normals, positions, droughness, dcolor), dim=1) # positions two times, idk why it doesnt work with out it!!!!!! TO DO\n",
    "        \n",
    "\n",
    "\n",
    "def aces_tonemap(image, exposure=1.0):\n",
    "    \"\"\"\n",
    "    ACES tonemapping function.\n",
    "    :param image: Input HDR image.\n",
    "    :param exposure: Exposure factor.\n",
    "    :return: Tonemapped image.\n",
    "    \"\"\"\n",
    "    a = 2.51\n",
    "    b = 0.03\n",
    "    c = 2.43\n",
    "    d = 0.59\n",
    "    e = 0.14\n",
    "\n",
    "    mapped = exposure * image\n",
    "    return ((mapped * (a * mapped + b)) / (mapped * (c * mapped + d) + e))\n",
    "\n",
    "\n",
    "def apply_gamma_correction(image):\n",
    "    \"\"\"\n",
    "    Applies gamma correction to the image.\n",
    "    :param image: Input image to correct.\n",
    "    :return: Gamma corrected image.\n",
    "    \"\"\"\n",
    "    gamma = 2.2\n",
    "    return np.power(image, 1.0 / gamma)\n",
    "\n",
    "\n",
    "def cart_to_sph(tensor):\n",
    "    theta = torch.acos(tensor[:,2])  # arccos(z/r)\n",
    "    phi = torch.atan2(tensor[:,1], tensor[:,0])  # arctan(y/x)\n",
    "\n",
    "    theta = theta / torch.tensor(np.pi)\n",
    "    phi = (phi/torch.tensor(np.pi))*0.5 + 0.5\n",
    "    return torch.stack([theta, phi], dim=1)\n",
    "\n",
    "\n",
    "def vectorized_uniform_stratified_sampling(width, height, tile_width, tile_height):\n",
    "    # Calculate the number of tiles in each dimension\n",
    "    num_tiles_x = width // tile_width\n",
    "    num_tiles_y = height // tile_height\n",
    "\n",
    "    # Generate grid indices for tiles\n",
    "    tiles_x = torch.arange(num_tiles_x, device=device_t).repeat(num_tiles_y)\n",
    "    tiles_y = torch.arange(num_tiles_y, device=device_t).repeat_interleave(num_tiles_x)\n",
    "\n",
    "    # Generate random points within each tile\n",
    "    points_within_tile_x = torch.randint(0, tile_width, (num_tiles_x * num_tiles_y,), device=device_t)\n",
    "    points_within_tile_y = torch.randint(0, tile_height, (num_tiles_x * num_tiles_y,), device=device_t)\n",
    "\n",
    "    # Calculate global indices\n",
    "    global_indices = ((tiles_y * tile_height + points_within_tile_y) * width + (tiles_x * tile_width + points_within_tile_x))\n",
    "\n",
    "    return global_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "def l2_regularization(model, lambda_l2):\n",
    "    l2_loss = torch.tensor(0.0).to(device_t)\n",
    "    for param in model.parameters():\n",
    "        l2_loss += torch.norm(param)**2\n",
    "    return lambda_l2 * l2_loss\n",
    "\n",
    "# assume one pdf for both prediction and ref, that these samples have been already divided by pdf!! pdf must be 1 for delta surfaces!!\n",
    "def relativeL2(prediction, ref, pdf=None, eps=None, div=None):\n",
    "    eps = gparams.relative_error_eps if eps is None else eps\n",
    "    div = prediction.detach() if div is None else div\n",
    "    denominator = torch.mean(div, dim=1).view(-1,1)**2 +eps\n",
    "    \n",
    "\n",
    "    rL2 = (pdf*((prediction - ref))**2) / denominator\n",
    "    rL2 = rL2[~(torch.isinf(rL2) | torch.isnan(rL2))]\n",
    "    rL2 = rL2.mean()\n",
    "    return rL2 \n",
    "\n",
    "\n",
    "def relativeL2Special(prediction, ref, pdf=None,  div=None):\n",
    "    rL2 = (pdf*((prediction - ref))**2) /  div\n",
    "    rL2 = rL2[~(torch.isinf(rL2) | torch.isnan(rL2))]\n",
    "    rL2 = rL2.mean()\n",
    "    return rL2 \n",
    "\n",
    "def relativeL2PDFCounted(prediction, ref, pdf=None, eps=None, div=None):\n",
    "    eps = gparams.relative_error_eps if eps is None else eps\n",
    "    div = prediction.detach() if div is None else div\n",
    "    denominator = torch.mean(prediction.detach(), dim=1).view(-1,1)**2 +eps\n",
    "    \n",
    "\n",
    "    rL2 = (((prediction - ref))**2) / (denominator*pdf)\n",
    "    rL2 = rL2[~(torch.isinf(rL2) | torch.isnan(rL2))]\n",
    "    rL2 = rL2.mean()\n",
    "    return rL2 \n",
    "\n",
    "# assume one pdf for both prediction and ref, that these samples have been already divided by pdf!! pdf must be 1 for delta surfaces!!\n",
    "def relativeL22(prediction, ref, pdf=None, eps=None, div=None):\n",
    "    eps = gparams.relative_error_eps if eps is None else eps\n",
    "    div = prediction.detach() if div is None else div\n",
    "    denominator = torch.mean(prediction.detach(), dim=1).view(-1,1)**2 +eps\n",
    "    \n",
    "\n",
    "    rL2 = (((prediction - ref))**2) / (denominator*pdf)\n",
    "    rL2 = rL2[~(torch.isinf(rL2) | torch.isnan(rL2))]\n",
    "    rL2 = rL2.mean()\n",
    "    return rL2 \n",
    "\n",
    "\n",
    "# we transform delta pdfs from 0 to 1.0\n",
    "def safe_pdf(pdf):\n",
    "    pdf = torch.where(pdf <= 0.000000000, torch.tensor(1.0), pdf)\n",
    "    return pdf\n",
    "\n",
    "# assume pne pdf for both prediction and ref, but only ref has been divided by pdf \n",
    "def relativeL2_PDF_NotApplied(prediction, ref, pdf=None, eps=None, div=None):\n",
    "    eps = gparams.relative_error_eps if eps is None else eps\n",
    "    div = prediction.detach() if div is None else div\n",
    "    denominator = torch.mean(prediction.detach(), dim=1).view(-1,1)**2 +eps\n",
    "    sqrt_pdf = torch.sqrt(pdf)\n",
    "\n",
    "    rL2 = (((prediction - ref*pdf))**2) / (denominator*pdf)\n",
    "    rL2 = rL2[~(torch.isinf(rL2) | torch.isnan(rL2))]\n",
    "    rL2 = rL2.mean()\n",
    "    return rL2 \n",
    "\n",
    "\n",
    "def L2(prediction, ref, pdf=1):\n",
    "    rL2 = pdf*(prediction - ref)**2\n",
    "    rL2 = rL2.mean()\n",
    "    return rL2 \n",
    "\n",
    "def aces_tonemap(image, exposure=1.0):\n",
    "    \"\"\"\n",
    "    ACES tonemapping function.\n",
    "    :param image: Input HDR image.\n",
    "    :param exposure: Exposure factor.\n",
    "    :return: Tonemapped image.\n",
    "    \"\"\"\n",
    "    a = 2.51\n",
    "    b = 0.03\n",
    "    c = 2.43\n",
    "    d = 0.59\n",
    "    e = 0.14\n",
    "\n",
    "    mapped = exposure * image\n",
    "    return ((mapped * (a * mapped + b)) / (mapped * (c * mapped + d) + e))\n",
    "\n",
    "def apply_gamma_correction(image):\n",
    "    \"\"\"\n",
    "    Applies gamma correction to the image.\n",
    "    :param image: Input image to correct.\n",
    "    :return: Gamma corrected image.\n",
    "    \"\"\"\n",
    "    gamma = 2.2\n",
    "    image = np.clip(image, 0.0, 1.0)\n",
    "    image = np.power(image, 1.0 / gamma)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = np.clip(image, 0, 255)\n",
    "    return image\n",
    "\n",
    "# model(X).cpu()\n",
    "def visualize_model_output(Y, width, height, epoch, tonemap_gamma=True):\n",
    "    with torch.no_grad():\n",
    "        output = Y.numpy().astype(np.float32)\n",
    "        output_image = output.reshape(height, width, 3)\n",
    "\n",
    "        if tonemap_gamma:\n",
    "            output_image = apply_gamma_correction(aces_tonemap(output_image))\n",
    "        \n",
    "        plt.imshow(output_image)\n",
    "        plt.title(f'Model Output at Epoch {epoch + 1}')\n",
    "        plt.show()\n",
    "\n",
    "# Function to determine if visualization should occur at the current epoch\n",
    "def should_visualize(epoch, conditions):\n",
    "    for max_epoch, interval in conditions:\n",
    "        if epoch < max_epoch:\n",
    "            return (epoch + 1) % interval == 0\n",
    "    return (epoch + 1) % 200 == 0\n",
    "\n",
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "# Define the MLP class\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=6):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.model.apply(self.init_weights)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PixelLatentLayer(nn.Module):\n",
    "    def __init__(self, width, height, params_per_pixel, device, init_method='xavier'):\n",
    "        super(PixelLatentLayer, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.params_per_pixel = params_per_pixel\n",
    "        self.latents_per_pixel = nn.Parameter(torch.empty(width, height, params_per_pixel, device=device))\n",
    "\n",
    "        if init_method == 'xavier':\n",
    "            bound = math.sqrt(2 / (64 + params_per_pixel))\n",
    "        elif init_method == 'he':\n",
    "            bound = math.sqrt(2 / (64))\n",
    "        elif init_method == 'uniform_large':\n",
    "            bound = 1e-4\n",
    "        else:\n",
    "            bound = 1e-4  # default\n",
    "\n",
    "        nn.init.uniform_(self.latents_per_pixel, -bound, bound)\n",
    "\n",
    "    def forward(self, pixel_indices):\n",
    "        latents = self.latents_per_pixel[pixel_indices[:, 0], pixel_indices[:, 1]]\n",
    "        return latents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the ref_estimations tensor with zeros of required shape\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_path = os.path.join(scene_cfg.checkpoint_dir, f'ref_estimations_{gparams.render_width}x{gparams.render_height}.pth')\n",
    "\n",
    "# Try to load cached ref_estimations if it exists\n",
    "found = False\n",
    "if os.path.exists(checkpoint_path):\n",
    "    found = True\n",
    "    ref_estimations = torch.load(checkpoint_path, map_location=device_t)\n",
    "    print(\"Loaded cached ref_estimations from checkpoint.\")\n",
    "else:\n",
    "    print(\"No cached ref_estimations found. Initializing new tensor.\")\n",
    "\n",
    "#found = False\n",
    "scene_cfg.prepare_scene(ref_scene)\n",
    "if not found:\n",
    "    for epoch in range(4000):\n",
    "        start_render = time.time()\n",
    "        # Render a new frame each epoch\n",
    "        trainFrame()\n",
    "\n",
    "        # Get the newly rendered data\n",
    "        mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "\n",
    "        y = mlDataOutput[\"radiance\"]+0.0\n",
    "\n",
    "        mask = get_mask_for_training(y, False)        \n",
    "        y[~mask] = 0.0\n",
    "\n",
    "        if epoch == 0:\n",
    "            ref_estimations = y\n",
    "        else:\n",
    "            a = 1.0 / epoch\n",
    "            ref_estimations = y * a + ref_estimations * (1.0 - a)\n",
    "\n",
    "        # Save the ref_estimations tensor every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            torch.save(ref_estimations, checkpoint_path)\n",
    "            print(f\"Saved ref_estimations at epoch {epoch + 1}.\")\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            visualize_model_output(ref_estimations.cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "else:\n",
    "    visualize_model_output(ref_estimations.cpu(), gparams.render_width, gparams.render_height, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceHelper:\n",
    "    def __init__(self, width, height, device, alpha=0.95):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.device = device\n",
    "        self.m1 = torch.zeros((width * height, 3), device=device)\n",
    "        self.m2 = torch.zeros((width * height, 3), device=device)\n",
    "        self.epochs = torch.zeros((width * height), dtype=torch.int32, device=device)\n",
    "        self.epochs_ema = torch.zeros((width * height), dtype=torch.int32, device=device)\n",
    "        self.alpha = alpha  # Smoothing factor for EMA\n",
    "        self.m1_ema = torch.zeros((width * height, 3), device=device)\n",
    "        self.m2_ema = torch.zeros((width * height, 3), device=device)\n",
    "        self.variance_per_epoch = []  # Store (epoch, variance) tuples\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'm1': self.m1,\n",
    "            'm2': self.m2,\n",
    "            'epochs': self.epochs,\n",
    "            'epochs_ema': self.epochs_ema,\n",
    "            'm1_ema': self.m1_ema,\n",
    "            'm2_ema': self.m2_ema,\n",
    "            'alpha': self.alpha,\n",
    "            'variance_per_epoch': self.variance_per_epoch\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.m1 = state_dict['m1'].to(self.device)\n",
    "        self.m2 = state_dict['m2'].to(self.device)\n",
    "        self.epochs = state_dict['epochs'].to(self.device)\n",
    "        self.epochs_ema = state_dict['epochs_ema'].to(self.device)\n",
    "        self.m1_ema = state_dict['m1_ema'].to(self.device)\n",
    "        self.m2_ema = state_dict['m2_ema'].to(self.device)\n",
    "        self.alpha = state_dict['alpha']\n",
    "        self.variance_per_epoch = state_dict['variance_per_epoch']\n",
    "\n",
    "    def update(self, indices, bias):\n",
    "        \"\"\"\n",
    "        Update the incremental variance estimators with new bias data.\n",
    "        Skips updates for entries where bias contains NaN or Infinity.\n",
    "        \"\"\"\n",
    "        # Ensure bias has the correct shape\n",
    "        if bias.dim() != 2 or bias.size(0) != indices.size(0):\n",
    "            raise ValueError(\"Bias tensor must be 2-dimensional with shape (N, 3) matching the number of indices.\")\n",
    "\n",
    "        # Create a mask for valid (finite) bias entries\n",
    "        valid_mask = torch.isfinite(bias).all(dim=1)\n",
    "        if not valid_mask.any():\n",
    "            # If no valid data, exit early\n",
    "            return\n",
    "\n",
    "        # Select only valid indices and corresponding bias\n",
    "        valid_indices = indices[valid_mask]\n",
    "        valid_bias = bias[valid_mask]\n",
    "\n",
    "        # Get the current epoch for valid masked pixels\n",
    "        current_epochs = self.epochs[valid_indices]\n",
    "\n",
    "        # Compute weight for valid masked pixels\n",
    "        weights = 1.0 / (current_epochs.float() + 1)\n",
    "        weights = weights.unsqueeze(1)  # Make it (N, 1) to match bias shape\n",
    "\n",
    "        # Update m1 and m2 only for valid masked pixels (incremental estimator)\n",
    "        self.m1[valid_indices] = valid_bias * weights + (1 - weights) * self.m1[valid_indices]\n",
    "        self.m2[valid_indices] = (valid_bias ** 2) * weights + (1 - weights) * self.m2[valid_indices]\n",
    "\n",
    "        # Update EMA m1 and m2 for valid masked pixels (EMA estimator)\n",
    "        self.m1_ema[valid_indices] = self.alpha * self.m1_ema[valid_indices] + (1 - self.alpha) * valid_bias\n",
    "        self.m2_ema[valid_indices] = self.alpha * self.m2_ema[valid_indices] + (1 - self.alpha) * (valid_bias ** 2)\n",
    "\n",
    "        # Increment the epoch counter for valid masked pixels\n",
    "        self.epochs[valid_indices] += 1\n",
    "\n",
    "    def update_ema(self, bias, indices=None):\n",
    "        \"\"\"\n",
    "        Update the EMA-based variance estimators with new bias data.\n",
    "        Skips updates for entries where bias contains NaN or Infinity.\n",
    "        \"\"\"\n",
    "        if indices is None:\n",
    "            indices = torch.arange(self.m1_ema.shape[0], device=self.m1_ema.device)\n",
    "\n",
    "        # Ensure bias has the correct shape\n",
    "        if bias.dim() != 2 or bias.size(0) != indices.size(0):\n",
    "            raise ValueError(\"Bias tensor must be 2-dimensional with shape (N, 3) matching the number of indices.\")\n",
    "\n",
    "        # Create a mask for valid (finite) bias entries\n",
    "        valid_mask = torch.isfinite(bias).all(dim=1)\n",
    "        if not valid_mask.any():\n",
    "            # If no valid data, exit early\n",
    "            return\n",
    "\n",
    "        # Select only valid indices and corresponding bias\n",
    "        valid_indices = indices[valid_mask]\n",
    "        valid_bias = bias[valid_mask]\n",
    "\n",
    "        # Create a tensor version of alpha\n",
    "        alpha_tensor = torch.ones_like(self.m1_ema[valid_indices]) * self.alpha\n",
    "        alpha_tensor[self.epochs_ema[valid_indices] == 0] = 0\n",
    "\n",
    "        # Update EMA m1 and m2 for valid masked pixels (EMA estimator)\n",
    "        self.m1_ema[valid_indices] = alpha_tensor * self.m1_ema[valid_indices] + (1 - alpha_tensor) * valid_bias\n",
    "        self.m2_ema[valid_indices] = alpha_tensor * self.m2_ema[valid_indices] + (1 - alpha_tensor) * (valid_bias ** 2)\n",
    "\n",
    "        # Increment the epoch counter for valid masked pixels\n",
    "        self.epochs_ema[valid_indices] += 1\n",
    "\n",
    "\n",
    "\n",
    "    def get_variance(self, estimations=ref_estimations, eps=gparams.relative_error_eps):\n",
    "        # Compute variance using the incremental estimator\n",
    "        variance = (self.m2 - self.m1 ** 2) / (estimations ** 2 + eps)\n",
    "        # Return mean variance where epochs >= 1\n",
    "        return torch.mean(variance[self.epochs >= 1])\n",
    "\n",
    "    def get_variance_ema(self, estimations=ref_estimations, eps=gparams.relative_error_eps, epoch=None):\n",
    "        # Compute variance using the EMA estimator\n",
    "        variance_ema = (self.m2_ema - self.m1_ema ** 2) / (estimations ** 2 + eps)\n",
    "        # Calculate mean variance where epochs_ema >= 1\n",
    "        mean_variance_ema = torch.mean(variance_ema[self.epochs_ema >= 1])\n",
    "        \n",
    "        # Cache the epoch and mean variance for this epoch\n",
    "        if epoch is not None:\n",
    "            self.variance_per_epoch.append((epoch, mean_variance_ema.item()))\n",
    "        \n",
    "        return mean_variance_ema.item()\n",
    "    \n",
    "    def get_cached_variance_per_epoch(self):\n",
    "        return self.variance_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateVarianceBaseline(steps=1000, recompute=False):\n",
    "    # Define checkpoint path for saving and loading variance data\n",
    "    checkpoint_path = os.path.join(scene_cfg.checkpoint_dir, 'ref_variance.pth')\n",
    "\n",
    "    # Check if recompute is set to False and the checkpoint file exists\n",
    "    if not recompute and os.path.exists(checkpoint_path):\n",
    "        # Load the variance data from checkpoint\n",
    "        scene_cfg.ref_variance = torch.load(checkpoint_path)\n",
    "        print(f\"Loaded variance from {checkpoint_path}: {scene_cfg.ref_variance}\")\n",
    "        return\n",
    "\n",
    "    # Else, perform variance computation\n",
    "    variance_helper = VarianceHelper(gparams.render_width, gparams.render_height, device=device_t)\n",
    "\n",
    "    for i in range(steps):\n",
    "        trainFrame()  # Call to train frame\n",
    "\n",
    "        # Get machine learning data and split it accordingly\n",
    "        mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "        radiance = mlDataOutput[\"radiance\"] + 0  # Assuming radiance is a tensor of shape [N, 3]\n",
    "\n",
    "        # Find valid indices where radiance values are >= 0 and no inf or nan for all RGB channels\n",
    "        indices = torch.all((radiance >= 0) & torch.isfinite(radiance), dim=1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Select the valid radiance values\n",
    "        radiance = radiance[indices]\n",
    "\n",
    "        # Update the variance helper\n",
    "        variance_helper.update(indices=indices, bias=radiance)\n",
    "        print(f\"Variance Estimation {i}: {variance_helper.get_variance()}\")\n",
    "\n",
    "    # Get the final variance after the loop\n",
    "    final_variance = variance_helper.get_variance()\n",
    "\n",
    "    # Assign the computed variance to scene_cfg.ref_variance\n",
    "    scene_cfg.ref_variance = final_variance\n",
    "\n",
    "    # Save the final variance to the checkpoint file\n",
    "    torch.save(final_variance, checkpoint_path)\n",
    "    print(f\"Saved computed variance to {checkpoint_path}: {scene_cfg.ref_variance}\")\n",
    "\n",
    "# Example of calling the function\n",
    "updateVarianceBaseline(steps=scene_cfg.var_est_steps, recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zunis.models.flows.sampling import UniformSampler\n",
    "from zunis.training.weighted_dataset.stateful_trainer import StatefulTrainer\n",
    "\n",
    "\n",
    "class NCVModel(torch.nn.Module):\n",
    "    def __init__(self, width, height, params_per_pixel, config_path, integral_model=True, color=True, encoding_mode=\"world\", coord_system=\"spherical\", optional_features=True, model=\"default\", count_transf_pdf=True):\n",
    "        super(NCVModel, self).__init__()\n",
    "\n",
    "        num_dims = 2\n",
    "\n",
    "        self.encoding_mode = encoding_mode\n",
    "\n",
    "        self.world_encoder = None\n",
    "        self.screen_encoder = None\n",
    "        self.variance_helper = VarianceHelper(width, height, device=device_t)\n",
    "\n",
    "        num_feature_dims = 0\n",
    "        if self.encoding_mode == \"world\":\n",
    "            with open(config_path) as config_file:\n",
    "                config = json.load(config_file)\n",
    "            \n",
    "            encoding = tcnn.Encoding(11, config[\"encoding\"]).to(device_t)\n",
    "            halftofloat = LambdaLayer(lambda x: x.float())\n",
    "            self.world_encoder = torch.nn.Sequential(encoding, halftofloat)\n",
    "            #num_feature_dims = 47\n",
    "            num_feature_dims = encoding.n_output_dims\n",
    "        elif self.encoding_mode == \"screen\":\n",
    "            self.params_per_pixel = params_per_pixel\n",
    "            self.screen_encoder = PixelLatentLayer(width, height, params_per_pixel, device_t, init_method=\"default\")\n",
    "            num_feature_dims = params_per_pixel\n",
    "        else:\n",
    "            assert(0)\n",
    "\n",
    "\n",
    "        if integral_model == True:\n",
    "            self.integral_model = MLP(num_feature_dims, 3, num_layers=1).to(device_t)\n",
    "        else:\n",
    "            self.integral_model = None\n",
    "\n",
    "\n",
    "        self.color = color\n",
    "\n",
    "        if coord_system == \"cylindrical\":\n",
    "            num_dims = 3\n",
    "\n",
    "        \n",
    "        sampler = UniformSampler(d=num_dims, device=device_t)\n",
    "        self.optional_features = optional_features\n",
    "        self.coord_system = coord_system\n",
    "        #flow_options = {'cell_params': {'d_opt_feats': num_feature_dims * self.optional_features + 0, \"n_bins\": 64, \"model\": model}, \"masking_options\": {\"repetitions\": 2}}\n",
    "        #FIIIX\n",
    "        flow_options = {'cell_params': {'d_opt_feats': num_feature_dims * self.optional_features + 0, \"n_bins\": 64, \"model\": model, \"d_hidden\": 64, \"n_hidden\": 6}, \"masking_options\": {\"repetitions\": 2}}\n",
    "\n",
    "        num_channels = 3 if self.color else 1\n",
    "        self.trainers = [StatefulTrainer(d=num_dims, loss=\"l2\", flow_options=flow_options, flow=\"pwquad\", flow_prior=sampler, device=device_t) for _ in range(num_channels)]\n",
    "        \n",
    "                # Collect parameters into separate groups\n",
    "        flow_params = [param for trainer in self.trainers for param in trainer.flow.parameters()]\n",
    "\n",
    "        # Collect other parameters\n",
    "        other_params = []\n",
    "        if self.integral_model is not None:\n",
    "            other_params.extend(self.integral_model.parameters())\n",
    "\n",
    "        if self.world_encoder is not None:\n",
    "            other_params.extend(self.world_encoder.parameters())\n",
    "\n",
    "        if self.screen_encoder is not None:\n",
    "            other_params.extend(self.screen_encoder.parameters())\n",
    "\n",
    "        # Prepare parameter groups with different learning rates\n",
    "        param_groups = [\n",
    "            {'params': flow_params, 'lr': 0.00025 * scene_cfg.lr_factor, 'eps':1e-15}\n",
    "        ]\n",
    "\n",
    "        if other_params:\n",
    "            param_groups.append({'params': other_params, 'lr': 0.001 * scene_cfg.lr_factor, 'eps':1e-15})\n",
    "\n",
    "        # Create the optimizer with parameter groups\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            param_groups\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "        for trainer in self.trainers:\n",
    "            print(f\"NN params {count_parameters(trainer.flow)}\")        \n",
    "        print(f\"NCV params {count_parameters(self.integral_model)}\")        \n",
    "        print(f\"NCV params {count_parameters(self.world_encoder)}\")       \n",
    "\n",
    "        self.checkpoint_path = None\n",
    "        self.loss = 0\n",
    "        self.device = device_t\n",
    "        self.count_transf_pdf = count_transf_pdf\n",
    "\n",
    "        if not optional_features:\n",
    "            print(\"Warning, the model is training without the features. use it only for 1 pixel training!\")\n",
    "\n",
    "    def set_checkpoint_path(self, checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        name = 'ncv_checkpoint.pth'\n",
    "        if self.integral_model is not None:\n",
    "            name = 'ncv_nrc_checkpoint.pth'\n",
    "\n",
    "        self.checkpoint_path = os.path.join(checkpoint_dir, name)\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        if self.checkpoint_path:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'flow_state_dicts': [trainer.flow.state_dict() for trainer in self.trainers],\n",
    "                'flow_inverses': [trainer.flow.inverse for trainer in self.trainers],\n",
    "                'flows_inverses': [\n",
    "                    [flow.transform.inverse for flow in trainer.flow.flows]\n",
    "                    for trainer in self.trainers\n",
    "                ],\n",
    "                'loss': loss,\n",
    "                'rL2': self.rL2,\n",
    "                'variance_helper_state': self.variance_helper.state_dict()\n",
    "            }\n",
    "\n",
    "            if self.integral_model is not None:\n",
    "                checkpoint['integral_model_state_dict'] = self.integral_model.state_dict()\n",
    "            torch.save(checkpoint, self.checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, skip=False):\n",
    "        if not skip and self.checkpoint_path and os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)\n",
    "\n",
    "            self.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'integral_model_state_dict' in checkpoint:\n",
    "                self.integral_model.load_state_dict(checkpoint['integral_model_state_dict'])\n",
    "\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "            flow_state_dicts = checkpoint['flow_state_dicts']\n",
    "            flows_inverses = checkpoint['flows_inverses']\n",
    "            flow_inverses = checkpoint.get('flow_inverses', [True] * len(self.trainers))  # Default to False if not present\n",
    "\n",
    "            print(flows_inverses)\n",
    "\n",
    "            for i in range(len(self.trainers)):\n",
    "                # Load flow state dict\n",
    "                self.trainers[i].flow.load_state_dict(flow_state_dicts[i])\n",
    "                # Restore inversion state for each flow in the trainer\n",
    "                for j, inverse_flag in enumerate(flows_inverses[i]):\n",
    "                    self.trainers[i].flow.flows[j].transform.inverse = inverse_flag\n",
    "                self.trainers[i].flow.inverse = flow_inverses[i]\n",
    "\n",
    "            epoch = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "            self.rL2 = checkpoint.get('rL2', 0.0)  # Use get to provide a default value if 'rL2' isn't in the checkpoint\n",
    "\n",
    "            # Load VarianceHelper state\n",
    "            if 'variance_helper_state' in checkpoint:\n",
    "                self.variance_helper.load_state_dict(checkpoint['variance_helper_state'])\n",
    "\n",
    "            print(f\"Checkpoint loaded: Epoch {epoch}, Loss {loss}\")\n",
    "\n",
    "            return epoch, loss\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "\n",
    "    def toCylindrical(self, dirs):\n",
    "        rho = torch.sqrt(dirs[:, 0] ** 2 + dirs[:, 1] ** 2)\n",
    "        phi = torch.atan2(dirs[:, 1], dirs[:, 0])\n",
    "        z = dirs[:, 2]\n",
    "\n",
    "        rho_norm = rho\n",
    "        phi_norm = (phi + torch.pi) / (2 * torch.pi)\n",
    "        z_norm = (z + 1) / 2\n",
    "\n",
    "        cylindrical = torch.stack((rho_norm, phi_norm, z_norm), dim=1)\n",
    "        cylindrical = torch.clamp(cylindrical, 0.0, 1.0)\n",
    "\n",
    "        jacobian = rho / (4 * torch.pi)\n",
    "        jacobian = torch.clamp(jacobian, min=1e-6)\n",
    "\n",
    "        assert(check_tensor(cylindrical, \"cylindrical\", min=0.0, max=1.0))\n",
    "        assert(check_tensor(jacobian, \"jacobian\", min=1e-6))\n",
    "\n",
    "        return cylindrical, jacobian\n",
    "\n",
    "    def to2D(self, x):\n",
    "        return x, torch.ones((x.shape[0]), device=x.device)\n",
    "\n",
    "    def toSpherical(self, dirs):\n",
    "        x, y, z = dirs[:, 0], dirs[:, 1], dirs[:, 2]\n",
    "\n",
    "        theta = torch.atan2(torch.sqrt(x**2 + y**2), z)\n",
    "        phi = torch.atan2(y, x)\n",
    "\n",
    "        normalized_azimuth = (phi + np.pi) / (2 * np.pi)\n",
    "        normalized_theta = theta / np.pi\n",
    "\n",
    "        jacobian = torch.abs(torch.sin(theta))/(2 * np.pi * np.pi)\n",
    "\n",
    "        assert(check_tensor(normalized_azimuth, \"normalized_azimuth\", min=0.0, max=1.0))\n",
    "        assert(check_tensor(normalized_theta, \"normalized_theta\", min=0.0, max=1.0))\n",
    "\n",
    "        res = torch.stack([normalized_theta, normalized_azimuth], dim=1)\n",
    "        res = torch.clamp(res, 0.0, 1.0)\n",
    "        return res, jacobian\n",
    "\n",
    "    def transform_coordinates(self, x):\n",
    "        if self.coord_system == \"cylindrical\":\n",
    "            return self.toCylindrical(x)\n",
    "        elif self.coord_system == \"spherical\":\n",
    "            return self.toSpherical(x)\n",
    "        elif self.coord_system == \"2D\":\n",
    "            return self.to2D(x)\n",
    "        else:\n",
    "            assert(0)\n",
    "\n",
    "    def forward(self, x, jacobian, pixels, world_data, integral_values=None, surface_color=None, detach=False, pdf_b=None, only_integral=False):\n",
    "        assert(check_tensor(x, \"positions\", min=0.00000, max=1.0))\n",
    "\n",
    "        if not self.count_transf_pdf:\n",
    "            jacobian = jacobian * 0.0 + 1.0\n",
    "\n",
    "        check_tensor(torch.log(jacobian.unsqueeze(-1)), \"torch.log(pdf.unsqueeze(-1))\")\n",
    "\n",
    "        batch_size_limit = 256 * 256  # Maximum number of entries per batch\n",
    "        num_samples = x.shape[0]\n",
    "        outputs = []\n",
    "        estimations_list = []\n",
    "\n",
    "        for start_idx in range(0, num_samples, batch_size_limit):\n",
    "            end_idx = min(start_idx + batch_size_limit, num_samples)\n",
    "\n",
    "            # Slice the batch\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            jacobian_batch = jacobian[start_idx:end_idx]\n",
    "            pixels_batch = pixels[start_idx:end_idx] if pixels is not None else None\n",
    "            world_data_batch = world_data[start_idx:end_idx] if world_data is not None else None\n",
    "\n",
    "            xd_batch = torch.cat((x_batch, torch.log(jacobian_batch.unsqueeze(-1))), -1)\n",
    "\n",
    "            # Feature prefetching within the batch\n",
    "            if not self.optional_features:\n",
    "                features_batch = None\n",
    "            else:\n",
    "                features_list = []\n",
    "                if self.world_encoder is not None:\n",
    "                    features_world = self.world_encoder(world_data_batch)\n",
    "                    features_list.append(features_world)\n",
    "                if self.screen_encoder is not None:\n",
    "                    features_screen = self.screen_encoder(pixels_batch)\n",
    "                    features_list.append(features_screen)\n",
    "                if features_list:\n",
    "                    features_batch = torch.cat(features_list, dim=-1)\n",
    "                else:\n",
    "                    features_batch = None\n",
    "\n",
    "            # Integral model processing within the batch\n",
    "            if self.integral_model is None:\n",
    "                if integral_values is not None:\n",
    "                    estimation_batch = integral_values[start_idx:end_idx]\n",
    "                else:\n",
    "                    estimation_batch = None  # Set to zero or an appropriate default if necessary\n",
    "            else:\n",
    "                estimation_batch = torch.relu(self.integral_model(features_batch))\n",
    "                if detach:\n",
    "                    estimation_batch = estimation_batch.detach()\n",
    "\n",
    "            estimations_list.append(estimation_batch)\n",
    "\n",
    "            if only_integral:\n",
    "                continue  # Skip processing trainers if only the integral is needed\n",
    "\n",
    "            batch_predictions = []\n",
    "\n",
    "            for i, trainer in enumerate(self.trainers):\n",
    "                if not trainer.flow.inverse:\n",
    "                    trainer.flow.invert()\n",
    "\n",
    "                # Flow processing within the batch\n",
    "                zj = trainer.flow(xd_batch, opt_feats=features_batch)\n",
    "                if detach:\n",
    "                    zj = zj.detach()\n",
    "\n",
    "                z = zj[:, :-1]\n",
    "                logqx = zj[:, -1] + trainer.latent_prior.log_prob(z)\n",
    "                prediction = torch.exp(logqx).unsqueeze(-1)\n",
    "\n",
    "                # Apply the estimation from the integral model\n",
    "                if estimation_batch is not None:\n",
    "                    estimation_i = estimation_batch[:, i:i+1] if estimation_batch.ndim > 1 else estimation_batch.unsqueeze(-1)\n",
    "                    estimation_i = estimation_i.detach()\n",
    "                    prediction = prediction * estimation_i\n",
    "\n",
    "                if pdf_b is not None:\n",
    "                    prediction = prediction / pdf_b[start_idx:end_idx]\n",
    "\n",
    "                batch_predictions.append(prediction)\n",
    "\n",
    "            # Concatenate predictions from all trainers for this batch\n",
    "            prediction_batch = torch.cat(batch_predictions, dim=1)  # Shape: [batch_size, num_trainers]\n",
    "            outputs.append(prediction_batch)\n",
    "\n",
    "        if only_integral:\n",
    "            estimations = torch.cat(estimations_list, dim=0)\n",
    "            return None, estimations\n",
    "        else:\n",
    "            # Concatenate outputs from all batches\n",
    "            predictions = torch.cat(outputs, dim=0)  # Shape: [num_samples, num_trainers]\n",
    "            estimations = torch.cat(estimations_list, dim=0)  # Shape: [num_samples, num_trainers]\n",
    "            return predictions, estimations\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def prepare_input_(self, dirs, pixel_indices):\n",
    "        x, jacobian = self.transform_coordinates(dirs)\n",
    "\n",
    "        assert(check_tensor(x, \"x\", min=0.0, max=1.0, force_analyze=False))\n",
    "        assert(check_tensor(jacobian, \"jacobian\", min=0.0, max=1.0, force_analyze=False))\n",
    "        return x, jacobian, pixel_indices\n",
    "\n",
    "    def world_prepare_input_(self, dposition, dcolor, droughness, dnormal, dview, alpha=1.0):\n",
    "        normals = dnormal\n",
    "        normals = cart_to_sph(normals)\n",
    "\n",
    "        scene_min = torch.tensor(float3tonumpy(ref_scene.bounds.min_point), device=device_t)\n",
    "        scene_max = torch.tensor(float3tonumpy(ref_scene.bounds.max_point), device=device_t)\n",
    "        positions = (dposition - scene_min) / (scene_max - scene_min)\n",
    "\n",
    "\n",
    "        views = cart_to_sph(dview)\n",
    "\n",
    "        assert(check_tensor(normals, \"normals\",min=0.0, max=1.0))\n",
    "        assert(check_tensor(positions, \"positions\",min=0.0, max=1.0))\n",
    "        assert(check_tensor(dcolor, \"dcolor\",min=0.0, max=1.0))\n",
    "        assert(check_tensor(droughness, \"droughness\",min=0.0, max=1.0))\n",
    "        assert(check_tensor(views, \"dview\", min=0.0, max=1.0))\n",
    "\n",
    "        return torch.cat((normals*alpha, positions*alpha, droughness*alpha, views*alpha,dcolor), dim=1)\n",
    "\n",
    "    def world_prepare_input(self, mlDataOutput, alpha=1.0):\n",
    "        return self.world_prepare_input_(mlDataOutput[\"worldpos\"] + 0.0, mlDataOutput[\"color\"] + 0.0, mlDataOutput[\"roughness\"] + 0.0, mlDataOutput[\"normal\"] + 0.0, mlDataOutput[\"view\"] + 0.0, alpha=alpha)\n",
    "\n",
    "    def prepare_input(self, mlDataOutput, pixel_indices, alpha=1.0):\n",
    "        ddir = mlDataOutput[\"dir\"] + 0\n",
    "        pdf = mlDataOutput[\"pdf\"] + 0.0\n",
    "        pdf = safe_pdf(pdf)\n",
    "        x, jacobian, pixels = self.prepare_input_(ddir, pixel_indices)\n",
    "        world_input = self.world_prepare_input(mlDataOutput, alpha=alpha)\n",
    "        return pdf, x, jacobian, pixels, world_input\n",
    "\n",
    "    def prepare_output(self, mlDataOutput):\n",
    "        Radiance = mlDataOutput[\"radiance\"] + 0.0\n",
    "        return Radiance\n",
    "\n",
    "\n",
    "\n",
    "def train_ncv(ncv_model, tile_width,tile_height, B, N, start_epoch=0, integral_delay = 50, onlyIntegral=False, onlyRender=False, debug_train_id=None, var_bootstrap_steps=0):\n",
    "    scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "    average_kld_loss = 0.0\n",
    "    average_l2_loss = 0.0\n",
    "\n",
    "    relative_error = False\n",
    "    if start_epoch >= N:\n",
    "        if gparams.RECOMPUTE_LOSS[\"NCV\"]:\n",
    "            start_epoch = N\n",
    "            ncv_model.rL2 = 0\n",
    "        else:\n",
    "            onlyRender = True\n",
    "\n",
    "\n",
    "    re_accum = 1\n",
    "\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            for i in range(var_bootstrap_steps):\n",
    "                trainFrame()\n",
    "                mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "                # Prepare inputs\n",
    "                \n",
    "                Y = ncv_model.prepare_output(mlDataOutput)\n",
    "                SurfaceColor = mlDataOutput[\"color\"] + 0.0\n",
    "                SurfaceColor = SurfaceColor\n",
    "\n",
    "                THP = mlDataOutput[\"thp\"] + 0.0\n",
    "\n",
    "                arr = torch.arange(mlDataOutput[\"dir\"].shape[0])\n",
    "                pixel_indices = torch.stack((arr % gparams.render_width, arr // gparams.render_width), dim=1).to(device_t)\n",
    "                integral_values = ref_estimations\n",
    "\n",
    "                alpha = 1.0\n",
    "                PDF, X, Jacobian, Pixels, WorldData = ncv_model.prepare_input(mlDataOutput, pixel_indices, alpha=alpha)\n",
    "\n",
    "                res, _ = ncv_model(x=X, jacobian=Jacobian, world_data=WorldData, integral_values=integral_values, surface_color=SurfaceColor, pixels=Pixels, pdf_b=PDF, detach=True)\n",
    "                res = res.detach()\n",
    "                bias = res-Y\n",
    "                ncv_model.variance_helper.update_ema(indices=None, bias=bias)\n",
    "                print(f\"Initial Variance Estimation {i}, rVar: {ncv_model.variance_helper.get_variance_ema()}\")\n",
    "\n",
    "    loss_integral = 0\n",
    "    rL2Integral = 0\n",
    "    for epoch in range(start_epoch, N + scene_cfg.var_est_steps):\n",
    "\n",
    "        if ncv_model.integral_model is None:\n",
    "            onlyIntegral = False\n",
    "            integral_delay = 0\n",
    "\n",
    "        if debug_train_id is not None:\n",
    "            B = 1\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        trainFrame()\n",
    "        mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "\n",
    "        Y = ncv_model.prepare_output(mlDataOutput)\n",
    "\n",
    "        Y = torch.clamp(Y, -10000, 10000)\n",
    "        SurfaceColor = mlDataOutput[\"color\"] + 0.0\n",
    "        SurfaceColor = SurfaceColor\n",
    "\n",
    "        THP = mlDataOutput[\"thp\"] + 0.0\n",
    "        batch_size = 0\n",
    "        arr = torch.arange(mlDataOutput[\"dir\"].shape[0])\n",
    "        pixel_indices = torch.stack((arr % gparams.render_width, arr // gparams.render_width), dim=1).to(device_t)\n",
    "\n",
    "        alpha = 1.0\n",
    "        PDF, X, Jacobian, Pixels, WorldData = ncv_model.prepare_input(mlDataOutput, pixel_indices, alpha=alpha)\n",
    "\n",
    "\n",
    "        width, height = gparams.render_width, gparams.render_height\n",
    "        loss_val = 0\n",
    "\n",
    "\n",
    "        if (epoch) % 100 == 99 and epoch != start_epoch:\n",
    "            ncv_model.save_checkpoint(epoch, average_loss)\n",
    "\n",
    "        loss_epoch = epoch-N\n",
    "        bTrainNow = loss_epoch < 0\n",
    "\n",
    "        if not onlyRender:\n",
    "            for batch_index in range(B):\n",
    "                if bTrainNow:\n",
    "                    ncv_model.optimizer.zero_grad()\n",
    "\n",
    "                # Apply tiling to the current batch\n",
    "                batch_indices = vectorized_uniform_stratified_sampling(width, height, tile_width, tile_height)\n",
    "                y_batch_ = Y[batch_indices]\n",
    "                mask = get_mask_for_training(y_batch_, False)    \n",
    "                batch_indices = batch_indices[mask]\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                y = Y[batch_indices]\n",
    "                thp = THP[batch_indices]\n",
    "                world_data = WorldData[batch_indices]\n",
    "                pixels = Pixels[batch_indices]\n",
    "                jacobian = Jacobian[batch_indices]\n",
    "                pdf = PDF[batch_indices]\n",
    "                x = X[batch_indices]\n",
    "                surface_color = SurfaceColor[batch_indices]\n",
    "\n",
    "                y = torch.clamp(y, -10000, 10000)\n",
    "                integral_values = ref_estimations[batch_indices]\n",
    "                batch_size = y.shape[0]\n",
    "                if debug_train_id is not None:\n",
    "                    x = torch.unsqueeze(X[debug_train_id], dim=0)\n",
    "                    world = torch.unsqueeze(WorldData[debug_train_id], dim=0)\n",
    "                    pixels = torch.unsqueeze(Pixels[debug_train_id], dim=0)\n",
    "                    pdf = torch.unsqueeze(PDF[debug_train_id], dim=0)\n",
    "                    y = torch.unsqueeze(Y[debug_train_id], dim=0)\n",
    "                    surface_color = torch.unsqueeze(SurfaceColor[debug_train_id], dim=0)\n",
    "                    integral_values = torch.unsqueeze(ref_estimations[debug_train_id], dim=0)\n",
    "\n",
    "                if debug_train_id is not None and torch.any(y < 0.0):\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                if True:\n",
    "                    mask = (pdf > 0.0)\n",
    "                    pdf[~mask] = 1.0\n",
    "                    pdf[mask] = pdf[mask] + 0.0001\n",
    "                \n",
    "                outputs, integral = ncv_model(x=x, jacobian=jacobian, world_data=world_data, integral_values=integral_values, surface_color=surface_color, pixels=pixels, pdf_b=pdf, detach=False)\n",
    "\n",
    "                divider = torch.mean(integral.detach(), dim=-1) **2 + gparams.relative_error_eps\n",
    "                if not bTrainNow:\n",
    "                    outputs = outputs.detach()\n",
    "                    loss_l2_v = 0\n",
    "\n",
    "\n",
    "                divider = divider.unsqueeze(-1)\n",
    "\n",
    "\n",
    "                if onlyIntegral:\n",
    "                    outputs = outputs.detach()\n",
    "\n",
    "                \n",
    "                loss = relativeL2Special(outputs, y, pdf=pdf, div=divider)\n",
    "\n",
    "                if onlyIntegral:\n",
    "                    loss = loss*0\n",
    "\n",
    "                \n",
    "\n",
    "                        \n",
    "                if epoch >= integral_delay:\n",
    "                    bias = outputs.detach()-y\n",
    "                    if not bTrainNow:\n",
    "                        ncv_model.variance_helper.update(indices=batch_indices, bias=bias)\n",
    "                    else:\n",
    "                        ncv_model.variance_helper.update_ema(indices=batch_indices, bias=bias)\n",
    "\n",
    "                total_loss = loss\n",
    "                loss_val = loss.item()\n",
    "                rL2 = loss.item()\n",
    "\n",
    "                if ncv_model.integral_model is not None:\n",
    "                    loss_integral = relativeL2(integral, y, pdf=pdf*0.0+1.0)\n",
    "                    rL2Integral = loss_integral.item()\n",
    "                    if epoch >= integral_delay:\n",
    "                        total_loss = loss*0.8+loss_integral*0.2\n",
    "                    else:\n",
    "                        total_loss = loss_integral\n",
    "\n",
    "                if bTrainNow:\n",
    "                    if epoch == 0 and batch_index == 0:\n",
    "                        ncv_model.rL2 = rL2\n",
    "                    ncv_model.rL2 = rL2*0.90+ncv_model.rL2*0.10\n",
    "                else:\n",
    "                    overe = 1.0 / re_accum  \n",
    "                    ncv_model.rL2 = rL2* overe + (1-overe) * ncv_model.rL2\n",
    "                    re_accum += 1\n",
    "\n",
    "\n",
    "                if bTrainNow:\n",
    "                    total_loss.backward()\n",
    "                    #torch.nn.utils.clip_grad_norm_(ncv_model.parameters(), 1000)  # Clipping gradients to avoid explosion\n",
    "                    if ncv_model.integral_model is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(ncv_model.integral_model.parameters(), 3000)  # Clipping gradients to avoid explosion\n",
    "                    ncv_model.optimizer.step()\n",
    "                if onlyRender:\n",
    "                    break\n",
    "\n",
    "            #ncv_model.scheduler.step()\n",
    "            end_time = time.time()\n",
    "\n",
    "\n",
    "        if epoch == start_epoch:\n",
    "            average_loss = loss_val\n",
    "        else:\n",
    "            alpha = 0.95\n",
    "            average_loss = average_loss * alpha + (1.0 - alpha) * loss_val\n",
    "\n",
    "\n",
    "        if bTrainNow and epoch>=integral_delay:\n",
    "            var = ncv_model.variance_helper.get_variance_ema(epoch=epoch-integral_delay)\n",
    "        else:\n",
    "            var = ncv_model.variance_helper.get_variance()\n",
    "            \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{N}], Batch: {batch_size} AveragedLoss: {average_loss}, Loss: {loss_val}, rL2_loss: {ncv_model.rL2} rL2_integral: {rL2Integral} variance: {var}')\n",
    "\n",
    "        #max_epoch, interval\n",
    "        conditions = [\n",
    "                (10, 5),\n",
    "                (100, 10),\n",
    "                (500, 50),\n",
    "            ]\n",
    "        # Example usage in training loop\n",
    "        if should_visualize(epoch, conditions) and bTrainNow or onlyRender:\n",
    "            if ncv_model.integral_model is not None:\n",
    "                outputs, integral = ncv_model(x=X, jacobian=Jacobian, world_data=WorldData, integral_values=None, surface_color=SurfaceColor, pixels=Pixels, pdf_b=PDF, only_integral=True, detach=True)\n",
    "                visualize_model_output(integral.cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "                #visualize_model_output(Y.cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "\n",
    "        if onlyRender:\n",
    "            break\n",
    "\n",
    "        # Visualization conditions (if any)\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "if True:\n",
    "    ncv_model = NCVModel(width=gparams.render_width, height=gparams.render_height, config_path=\"data/ncv.json\", params_per_pixel=74, encoding_mode=\"world\", integral_model=True, coord_system=\"cylindrical\", optional_features=True, model=\"oneblob\", count_transf_pdf=False, color=True).to(device_t)\n",
    "    ncv_model.set_checkpoint_path(scene_cfg.checkpoint_dir)\n",
    "    start_epoch, _ = ncv_model.load_checkpoint(skip=True)\n",
    "    train_ncv(ncv_model, tile_width=4, tile_height=4, B=1, N=scene_cfg.epochs,start_epoch=start_epoch, var_bootstrap_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL MODELS\n",
    "\n",
    "\n",
    "## NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NRC_Model(torch.nn.Module):\n",
    "#     def __init__(self, config_path, device):\n",
    "#         super(NRC_Model, self).__init__()\n",
    "#         with open(config_path) as config_file:\n",
    "#             config = json.load(config_file)\n",
    "        \n",
    "#         encoding = tcnn.Encoding(n_input_dims, config[\"encoding\"])\n",
    "#         network = tcnn.Network(encoding.n_output_dims, n_output_dims, config[\"network\"])\n",
    "#         model = torch.nn.Sequential(encoding, network)\n",
    "\n",
    "#         self.model = tcnn.NetworkWithInputEncoding(\n",
    "#             n_input_dims=12, \n",
    "#             n_output_dims=3, \n",
    "#             encoding_config=config[\"encoding\"], \n",
    "#             network_config=config[\"network\"]\n",
    "#         ).to(device)\n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, betas=[0.9, 0.99], eps=1e-16)\n",
    "#         self.checkpoint_path = None\n",
    "#         self.device = device\n",
    "#         self.rL2 = None\n",
    "\n",
    "#     def set_checkpoint_path(self, checkpoint_dir):\n",
    "#         os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#         self.checkpoint_path = os.path.join(checkpoint_dir, f'nrc_checkpoint.pth')\n",
    "\n",
    "#     def save_checkpoint(self, epoch, loss):\n",
    "#         if self.checkpoint_path:\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': self.model.state_dict(),\n",
    "#                 'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "#                 'loss': loss,\n",
    "#                 'rL2': self.rL2,\n",
    "#             }, self.checkpoint_path)\n",
    "\n",
    "#     def load_checkpoint(self, skip=False):\n",
    "#         if not skip and self.checkpoint_path and os.path.exists(self.checkpoint_path):\n",
    "#             checkpoint = torch.load(self.checkpoint_path)\n",
    "#             self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#             self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#             epoch = checkpoint['epoch']\n",
    "#             loss = checkpoint['loss']\n",
    "#             self.rL2 = checkpoint['rL2']\n",
    "            \n",
    "#             print(f\"Checkpoint loaded: Epoch {epoch}, Loss {loss}\")\n",
    "#             return epoch, loss\n",
    "#         else:\n",
    "#             return 0, None\n",
    "\n",
    "#     def forward(self, x, color, d_radiance=0):\n",
    "#         return self.model(x)*color+d_radiance\n",
    "\n",
    "#     def prepare_input(self, mlDataOutput, camera_position_tensor):\n",
    "#         pdf = mlDataOutput[\"pdf\"]\n",
    "#         view_vectors = camera_position_tensor - mlDataOutput[\"worldpos\"]\n",
    "#         norm_view_vectors = view_vectors / torch.norm(view_vectors, dim=1, keepdim=True)\n",
    "#         X = packModelInput(mlDataOutput[\"worldpos\"], norm_view_vectors, mlDataOutput[\"color\"], mlDataOutput[\"roughness\"], mlDataOutput[\"normal\"])\n",
    "#         return X, pdf\n",
    "\n",
    "\n",
    "# # Assuming scene_cfg, gparams, ml_data, and other necessary variables are properly defined\n",
    "# scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "# # Initialize NRC_Model\n",
    "# nrc_model = NRC_Model(config_path=\"data/nrc.json\", device=device_t)\n",
    "# nrc_model.set_checkpoint_path(scene_cfg.checkpoint_dir)\n",
    "\n",
    "# # Load checkpoint if it exists\n",
    "# start_epoch, _ = nrc_model.load_checkpoint(skip=False)\n",
    "\n",
    "# # Training parameters\n",
    "# N = gparams.num_training_frames  # number of epochs\n",
    "# B = 16  # number of batches\n",
    "# simulate_tiles = False  # like in the original NRC work\n",
    "# tile_width, tile_height = 4, 4  # Sampling one point per 2x2 tile\n",
    "# eps = 0.001\n",
    "# lambda_l2 = 0.00000001  # Weight for L2 regularization\n",
    "# L2Reg = False\n",
    "\n",
    "# # Enter training mode\n",
    "# nrc_model.train()\n",
    "\n",
    "# # Convert camera position to PyTorch tensor\n",
    "# camera_position_tensor = torch.tensor(scene_cfg.camera_position, dtype=torch.float32).to(device_t)\n",
    "\n",
    "# relative_error = True\n",
    "\n",
    "# N = 0\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, N):\n",
    "#     start_render = time.time()\n",
    "#     # Render a new frame each epoch\n",
    "#     trainFrame()\n",
    "#     # Get the newly rendered data\n",
    "#     mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "\n",
    "#     thp = mlDataOutput[\"thp\"]\n",
    "#     render_time = time.time() - start_render\n",
    "\n",
    "#     # Prepare inputs using the new method\n",
    "#     X, pdf = nrc_model.prepare_input(mlDataOutput, camera_position_tensor)\n",
    "\n",
    "#     color = mlDataOutput[\"color\"]\n",
    "#     y = mlDataOutput[\"radiance\"].clone()  # Clone to manage memory\n",
    "\n",
    "#     direct_radiance =  mlDataOutput[\"dradiance\"]\n",
    "\n",
    "#     # y = y*thp\n",
    "\n",
    "#     width, height = gparams.render_width, gparams.render_height\n",
    "#     total_samples = X.size()[0]\n",
    "#     samples_per_batch = total_samples // B\n",
    "\n",
    "#     for batch_index in range(B):\n",
    "#         nrc_model.optimizer.zero_grad()\n",
    "\n",
    "#         # Apply tiling to the current batch\n",
    "#         batch_indices = vectorized_uniform_stratified_sampling(width, height, tile_width, tile_height)\n",
    "#         batch_indices = batch_indices[:samples_per_batch]  # Ensure the batch size matches\n",
    "\n",
    "#         batch_x = X[batch_indices]\n",
    "#         batch_y = y[batch_indices]\n",
    "#         batch_pdf = pdf[batch_indices]\n",
    "\n",
    "#         mask = get_mask_for_training(batch_y, False)\n",
    "#         batch_y = batch_y[mask]\n",
    "#         batch_x = batch_x[mask]\n",
    "#         batch_pdf = batch_pdf[mask]\n",
    "#         batch_color = color[batch_indices][mask]\n",
    "#         # Forward pass\n",
    "#         outputs = nrc_model(batch_x, batch_color)\n",
    "#         denominator = torch.norm(outputs.detach(), dim=1).view(-1,1) + gparams.relative_error_eps if gparams.relative_error else 1.0\n",
    "#         loss = ((outputs - batch_y)**2 / denominator).mean()\n",
    "#         # Add L2 regularization if enabled\n",
    "#         total_loss = loss\n",
    "\n",
    "\n",
    "#         denominator = torch.norm(ref_estimations[batch_indices][mask], dim=1).view(-1,1)+gparams.relative_error_eps\n",
    "#         nrc_model.rL2 = ((outputs.detach() - batch_y)**2 / denominator).mean()\n",
    "\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         total_loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(nrc_model.model.parameters(), 1000)  # Clipping gradients to avoid explosion\n",
    "#         nrc_model.optimizer.step()\n",
    "\n",
    "#     # Reporting\n",
    "#     if epoch == start_epoch:\n",
    "#         average_loss = loss.item()\n",
    "#     else:\n",
    "#         alpha = 0.95\n",
    "#         average_loss = average_loss * alpha + (1.0 - alpha) * loss.item()\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{N}], AveragedLoss: {average_loss}, Loss: {loss.item()}, rL2_loss: {nrc_model.rL2}')\n",
    "\n",
    "#     if (epoch + 1) % 500 == 0:\n",
    "#         nrc_model.eval()\n",
    "#         visualize_model_output(nrc_model(X, color).cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "#         nrc_model.train()\n",
    "\n",
    "\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         nrc_model.save_checkpoint(epoch, average_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NIRC_Model(torch.nn.Module):\n",
    "    def __init__(self, width, height, config_path, device, init_method='xavier'):\n",
    "        super(NIRC_Model, self).__init__()\n",
    "        \n",
    "        with open(config_path) as config_file:\n",
    "            config = json.load(config_file)\n",
    "\n",
    "        \n",
    "        encoding = tcnn.Encoding(12, config[\"encoding\"]).to(device)\n",
    "        halftofloat = LambdaLayer(lambda x: x.float())\n",
    "        mlp = MLP(encoding.n_output_dims, 3).to(device)\n",
    "        print(f\"NN params {count_parameters(mlp)}\")\n",
    "        self.model = torch.nn.Sequential(encoding, halftofloat, mlp)\n",
    "        self.variance_helper = VarianceHelper(width, height, device=device)\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001*scene_cfg.lr_factor, betas=[0.9, 0.99], eps=1e-15)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=200, gamma=0.5)\n",
    "        print(f\"NN params {count_parameters(self.model)}\")\n",
    "        self.checkpoint_path = None\n",
    "        self.device = device\n",
    "        self.rL2 = 0\n",
    "\n",
    "\n",
    "    def set_checkpoint_path(self, checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(checkpoint_dir, f'nirc_checkpoint.pth')\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        if self.checkpoint_path and not math.isnan(loss) and not math.isinf(loss) and loss > 0:\n",
    "            print(f\"model saved on epoch {epoch} and rL2: {self.rL2}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'variance_helper_state': self.variance_helper.state_dict(),\n",
    "                'loss': loss,\n",
    "                'rL2': self.rL2,\n",
    "            }, self.checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, skip=False):\n",
    "        if not skip and self.checkpoint_path and os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            epoch = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "            self.rL2 = checkpoint['rL2']\n",
    "            \n",
    "            if 'variance_helper_state' in checkpoint:\n",
    "                self.variance_helper.load_state_dict(checkpoint['variance_helper_state'])\n",
    "            \n",
    "            print(f\"Checkpoint loaded: Epoch {epoch}, Loss {self.rL2 }\")\n",
    "            return epoch, loss\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "    def forward(self, x, thp):\n",
    "        return torch.relu(self.model(x))*thp\n",
    "    \n",
    "    def prepare_input_(self, dposition, ddir, dcolor, droughness, dnormal):\n",
    "        norm = (torch.sqrt(torch.sum(ddir**2, dim=1))+0.000000001)\n",
    "        norm = torch.unsqueeze(norm, dim=1)\n",
    "        ddir = ddir/norm\n",
    "        dirs = ddir*0.5+0.5\n",
    "            \n",
    "        normals =  dnormal\n",
    "        normals = cart_to_sph(normals)\n",
    "\n",
    "        # torch.Size([3])\n",
    "        scene_min = torch.tensor(float3tonumpy(ref_scene.bounds.min_point), device=\"cuda:0\")\n",
    "        scene_max = torch.tensor(float3tonumpy(ref_scene.bounds.max_point), device=\"cuda:0\")\n",
    "\n",
    "        #torch.Size([147600, 3])\n",
    "        positions = (dposition-scene_min)/(scene_max-scene_min)\n",
    "\n",
    "        assert(check_tensor(dirs, \"dirs\"))\n",
    "        assert(check_tensor(normals, \"normals\"))\n",
    "        assert(check_tensor(positions, \"positions\"))\n",
    "        assert(check_tensor(dcolor, \"dcolor\"))\n",
    "        assert(check_tensor(droughness, \"droughness\"))\n",
    "\n",
    "        return torch.cat((normals, positions, droughness, dcolor, dirs), dim=1)\n",
    "\n",
    "\n",
    "    def prepare_input(self, mlDataOutput):\n",
    "        return self.prepare_input_(mlDataOutput[\"worldpos\"]+0.0, mlDataOutput[\"dir\"]+0.0, mlDataOutput[\"color\"]+0.0, mlDataOutput[\"roughness\"]+0.0, mlDataOutput[\"normal\"]+0.0)\n",
    "        \n",
    "\n",
    "# Assuming scene_cfg, gparams, ml_data, and other necessary variables are properly defined\n",
    "scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "# Initialize NIRC_Model\n",
    "nirc_model = NIRC_Model(init_method='default', config_path=\"data/nirc.json\", width=gparams.render_width, height=gparams.render_height, device=device_t)\n",
    "nirc_model.set_checkpoint_path(scene_cfg.checkpoint_dir)\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "start_epoch, _ = nirc_model.load_checkpoint(skip=True)\n",
    "\n",
    "# Training parameters\n",
    "N = scene_cfg.epochs  # number of epochs\n",
    "simulate_tiles = False  # like in the original NRC work\n",
    "tile_width, tile_height = 8, 8  # Sampling one point per 2x2 tile\n",
    "B = 4  # number of batches\n",
    "eps = 0.005\n",
    "lambda_l2 = 0.00000001  # Weight for L2 regularization\n",
    "L2Reg = False\n",
    "\n",
    "\n",
    "\n",
    "# Enter training mode\n",
    "nirc_model.train()\n",
    "onlyRender = False\n",
    "if start_epoch >= N:\n",
    "    if gparams.RECOMPUTE_LOSS[\"NIRC\"]:\n",
    "        start_epoch = N\n",
    "        nirc_model.rL2 = 0\n",
    "    else:\n",
    "        onlyRender = True\n",
    "\n",
    "\n",
    "res_average = None\n",
    "re_accum = 1\n",
    "\n",
    "\n",
    "if start_epoch == 0:\n",
    "    nirc_model.eval()\n",
    "    for i in range(100):\n",
    "        trainFrame()\n",
    "        mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "        # Prepare inputs\n",
    "        y = 0.0+mlDataOutput[\"radiance\"]  # Clone to manage memory\n",
    "        y = torch.clamp(y, -10000, 10000)\n",
    "        thp = mlDataOutput[\"thp\"]+0.000\n",
    "        X = nirc_model.prepare_input(mlDataOutput)\n",
    "        res = nirc_model(X, thp).detach()        \n",
    "        bias = res-y\n",
    "        nirc_model.variance_helper.update_ema(indices=None, bias=bias)\n",
    "        print(f\"Initial Variance Estimation {i}, rVar: {nirc_model.variance_helper.get_variance_ema()}\")\n",
    "        \n",
    "    nirc_model.train()\n",
    "\n",
    "for epoch in range(start_epoch, N + scene_cfg.var_est_steps):\n",
    "    start_render = time.time()\n",
    "    # Render a new frame each epoch\n",
    "    trainFrame()\n",
    "    # Get the newly rendered data\n",
    "    mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "\n",
    "    render_time = time.time() - start_render\n",
    "\n",
    "    # Prepare inputs\n",
    "    y = 0.0+mlDataOutput[\"radiance\"]  # Clone to manage memory\n",
    "    thp = mlDataOutput[\"thp\"]+0.000\n",
    "    pdf = mlDataOutput[\"pdf\"]+0.00\n",
    "    X = nirc_model.prepare_input(mlDataOutput)\n",
    "    width, height = gparams.render_width, gparams.render_height\n",
    "    total_samples = X.size()[0]\n",
    "    samples_per_batch = total_samples // B\n",
    "    loss_val = 0\n",
    "\n",
    "    loss_epoch = epoch-N\n",
    "    bTrainNow = loss_epoch < 0\n",
    "\n",
    "\n",
    "    if not onlyRender:\n",
    "        for batch_index in range(B):\n",
    "            if bTrainNow:\n",
    "                nirc_model.optimizer.zero_grad()\n",
    "\n",
    "            # Step 1: Obtain initial batch indices\n",
    "            batch_indices = vectorized_uniform_stratified_sampling(width, height, tile_width, tile_height)\n",
    "\n",
    "            # Step 2: Fetch batch_y to compute the mask\n",
    "            batch_y = y[batch_indices]\n",
    "\n",
    "            # Step 3: Compute the mask based on batch_y\n",
    "            mask = get_mask_for_training(batch_y, False)\n",
    "\n",
    "            # Step 4: Update the indices based on the mask\n",
    "            batch_indices = batch_indices[mask]\n",
    "\n",
    "            # Step 5: Fetch all the data for the batch using updated indices\n",
    "            batch_x = X[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "            batch_thp = thp[batch_indices]\n",
    "            batch_pdf = pdf[batch_indices]\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = nirc_model(batch_x, batch_thp)\n",
    "\n",
    "            if not bTrainNow:\n",
    "                outputs = outputs.detach()\n",
    "                loss_val = 0\n",
    "\n",
    "            # For paper correct use\n",
    "            # loss = relativeL2(outputs, batch_y, pdf=batch_pdf)\n",
    "            \n",
    "            # For comparison with Control Variates (SH, VMF, NCV), as the demand higher stability\n",
    "            loss = relativeL2(outputs, batch_y, pdf=batch_pdf, div=ref_estimations[batch_indices])\n",
    "\n",
    "            if True:\n",
    "                batch_y = torch.clamp(batch_y, -10000, 10000)\n",
    "                bias = outputs.detach()-batch_y\n",
    "                if not bTrainNow:\n",
    "                    nirc_model.variance_helper.update(indices=batch_indices, bias=bias)\n",
    "                else:\n",
    "                    nirc_model.variance_helper.update_ema(bias, indices=batch_indices)\n",
    "\n",
    "\n",
    "            total_loss = loss \n",
    "\n",
    "            loss_val = loss.item()\n",
    "            rL2 = loss.item()\n",
    "            if bTrainNow:\n",
    "                if epoch == 0 and batch_index == 0:\n",
    "                    nirc_model.rL2 = rL2\n",
    "                nirc_model.rL2 = rL2*0.90+nirc_model.rL2*0.10\n",
    "            else:\n",
    "                overe = 1.0 / re_accum\n",
    "                nirc_model.rL2 = rL2* overe + (1-overe) * nirc_model.rL2\n",
    "                re_accum += 1\n",
    "\n",
    "                    # Backward and optimize\n",
    "            if bTrainNow:\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(nirc_model.parameters(), 1000)  # Clipping gradients to avoid explosion\n",
    "                nirc_model.optimizer.step()\n",
    "            if onlyRender:\n",
    "                break\n",
    "\n",
    "        nirc_model.scheduler.step()\n",
    "\n",
    "    if bTrainNow:\n",
    "        var = nirc_model.variance_helper.get_variance_ema(epoch=epoch)\n",
    "    else:\n",
    "        var = nirc_model.variance_helper.get_variance()\n",
    "    # Reporting\n",
    "    if epoch == start_epoch:\n",
    "        average_loss = loss_val\n",
    "    else:\n",
    "        alpha = 0.95\n",
    "        average_loss = average_loss * alpha + (1.0 - alpha) * loss_val\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{N}], AveragedLoss: {average_loss}, Loss: {loss_val}, rL2_loss: {nirc_model.rL2} variance: {var}')\n",
    "\n",
    "    # if (loss_val == 0 or math.isinf(loss_val) or math.isnan(loss_val)) and bTrainNow:\n",
    "    #     start_epoch, _ = nirc_model.load_checkpoint(skip=False)\n",
    "    #     epoch = start_epoch\n",
    "    #     print(\"RESTART!\")\n",
    "    #     continue\n",
    "\n",
    "    conditions = [\n",
    "        (10, 5),\n",
    "        (50, 10),\n",
    "        (300, 50),\n",
    "    ]\n",
    "\n",
    "\n",
    "    if not bTrainNow and False:\n",
    "        nirc_model.eval()\n",
    "        res = nirc_model(X, thp).detach()      \n",
    "        if res_average is None:\n",
    "            res_average = res\n",
    "        else:\n",
    "            res_average = res_average*0.99+res*0.01\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            visualize_model_output(res_average.cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "        nirc_model.train()\n",
    "\n",
    "    # Example usage in training loop\n",
    "    if should_visualize(epoch, conditions) and bTrainNow or onlyRender:\n",
    "        nirc_model.eval()\n",
    "        res = nirc_model(X, thp).detach()        \n",
    "\n",
    "        visualize_model_output(res.cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "        nirc_model.train()\n",
    "\n",
    "    if (epoch) % 100 == 99:\n",
    "        nirc_model.save_checkpoint(epoch, average_loss)\n",
    "\n",
    "    if onlyRender:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIRC Equal Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SH Layer\n",
    "class SHLayer(nn.Module):\n",
    "    def __init__(self, num_bands=5):\n",
    "        super(SHLayer, self).__init__()\n",
    "        self.num_bands = num_bands\n",
    "\n",
    "    def forward(self, dirs):\n",
    "        x, y, z = dirs[:, 0], dirs[:, 1], dirs[:, 2]\n",
    "\n",
    "        # Compute necessary products and powers\n",
    "        xy = x * y\n",
    "        xz = x * z\n",
    "        yz = y * z\n",
    "        x2 = x * x\n",
    "        y2 = y * y\n",
    "        z2 = z * z\n",
    "        x4 = x2 * x2\n",
    "        y4 = y2 * y2\n",
    "        z4 = z2 * z2\n",
    "\n",
    "        # Initialize the tensor to store the SH features\n",
    "        batch_size = dirs.size(0)\n",
    "        num_features = 25  # Assuming 5 bands\n",
    "        sh_features = torch.zeros(batch_size, num_features, device=dirs.device)\n",
    "\n",
    "        # Band 0\n",
    "        sh_features[:, 0] = 0.28209479177387814\n",
    "\n",
    "        # Band 1\n",
    "        if self.num_bands > 1:\n",
    "            sh_features[:, 1] = -0.48860251190291987 * y\n",
    "            sh_features[:, 2] = 0.48860251190291987 * z\n",
    "            sh_features[:, 3] = -0.48860251190291987 * x\n",
    "\n",
    "        # Band 2\n",
    "        if self.num_bands > 2:\n",
    "            sh_features[:, 4] = 1.0925484305920792 * xy\n",
    "            sh_features[:, 5] = -1.0925484305920792 * yz\n",
    "            sh_features[:, 6] = 0.94617469575755997 * z2 - 0.31539156525251999\n",
    "            sh_features[:, 7] = -1.0925484305920792 * xz\n",
    "            sh_features[:, 8] = 0.54627421529603959 * (x2 - y2)\n",
    "\n",
    "        # Band 3\n",
    "        if self.num_bands > 3:\n",
    "            sh_features[:, 9] = 0.59004358992664352 * y * (-3 * x2 + y2)\n",
    "            sh_features[:, 10] = 2.8906114426405538 * xy * z\n",
    "            sh_features[:, 11] = 0.45704579946446572 * y * (1 - 5 * z2)\n",
    "            sh_features[:, 12] = 0.3731763325901154 * z * (5 * z2 - 3)\n",
    "            sh_features[:, 13] = 0.45704579946446572 * x * (1 - 5 * z2)\n",
    "            sh_features[:, 14] = 1.4453057213202769 * z * (x2 - y2)\n",
    "            sh_features[:, 15] = 0.59004358992664352 * x * (-x2 + 3 * y2)\n",
    "\n",
    "        # Band 4\n",
    "        if self.num_bands > 4:\n",
    "            sh_features[:, 16] = 2.5033429417967046 * xy * (x2 - y2)\n",
    "            sh_features[:, 17] = 1.7701307697799304 * yz * (-3 * x2 + y2)\n",
    "            sh_features[:, 18] = 0.94617469575756008 * xy * (7 * z2 - 1)\n",
    "            sh_features[:, 19] = 0.66904654355728921 * yz * (3 - 7 * z2)\n",
    "            sh_features[:, 20] = -3.1735664074561294 * z2 + 3.7024941420321507 * z4 + 0.31735664074561293\n",
    "            sh_features[:, 21] = 0.66904654355728921 * xz * (3 - 7 * z2)\n",
    "            sh_features[:, 22] = 0.47308734787878004 * (x2 - y2) * (7 * z2 - 1)\n",
    "            sh_features[:, 23] = 1.7701307697799304 * xz * (-x2 + 3 * y2)\n",
    "            sh_features[:, 24] = -3.7550144126950569 * x2 * y2 + 0.62583573544917614 * x4 + 0.62583573544917614 * y4\n",
    "\n",
    "        return sh_features\n",
    "\n",
    "# Define the combined model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, width, height, params_per_pixel, num_bands, device):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.params_per_pixel = params_per_pixel\n",
    "        self.pixel_latent_layer = PixelLatentLayer(width, height, params_per_pixel, device)\n",
    "        self.sh_layer = SHLayer(num_bands).to(device)\n",
    "        input_dim = params_per_pixel + 25  # PixelLatentLayer produces params_per_pixel features, SHLayer produces 25 features\n",
    "        self.mlp = MLP(input_dim, 3).to(device)\n",
    "        print(f\"NN params {count_parameters(self.mlp)}\")\n",
    "\n",
    "    def forward(self, directions, pixel_indices):\n",
    "        sh_features = self.sh_layer(directions)\n",
    "        latent_features = self.pixel_latent_layer(pixel_indices)\n",
    "        combined_features = torch.cat((latent_features, sh_features), dim=1)\n",
    "        output = self.mlp(combined_features)\n",
    "        return output\n",
    "\n",
    "\n",
    "class NIRC_EQMem_Model(torch.nn.Module):\n",
    "    def __init__(self, width, height, config_path, device, params_per_pixel=2, init_method='xavier'):\n",
    "        super(NIRC_EQMem_Model, self).__init__()\n",
    "        self.model = CombinedModel(width, height,params_per_pixel, 5, device)\n",
    "        #self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001*scene_cfg.lr_factor, betas=[0.9, 0.99], eps=1e-15)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001, betas=[0.9, 0.99], eps=1e-15)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=300, gamma=0.25)\n",
    "        self.params_per_pixel = params_per_pixel\n",
    "        self.checkpoint_path = None\n",
    "        self.device = device\n",
    "        self.rL2 = 0\n",
    "        self.variance_helper = VarianceHelper(width, height, device=device)\n",
    "\n",
    "\n",
    "    def set_checkpoint_path(self, checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(checkpoint_dir, f'nirc_eqmem_{self.params_per_pixel}_checkpoint.pth')\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        if self.checkpoint_path:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'variance_helper_state': self.variance_helper.state_dict(),\n",
    "                'loss': loss,\n",
    "                'rL2': self.rL2,\n",
    "            }, self.checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, skip=False):\n",
    "        if not skip and self.checkpoint_path and os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            epoch = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "            self.rL2 = checkpoint['rL2']\n",
    "            if 'variance_helper_state' in checkpoint:\n",
    "                self.variance_helper.load_state_dict(checkpoint['variance_helper_state'])\n",
    "            \n",
    "            print(f\"Checkpoint loaded: Epoch {epoch}, Loss {self.rL2 }\")\n",
    "            return epoch, loss\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "    def forward(self, x, pixel_indices, thp):\n",
    "        return torch.relu(self.model(x, pixel_indices))*thp\n",
    "\n",
    "    def prepare_input_(self, dirs, pixel_indices, mask=None):\n",
    "        ddir = dirs\n",
    "        norm = (torch.sqrt(torch.sum(ddir**2, dim=1))).unsqueeze(1)\n",
    "        ddir = ddir / norm\n",
    "        if mask is not None:\n",
    "            X = X[mask]\n",
    "            pixel_indices = pixel_indices[mask]\n",
    "\n",
    "        return ddir, pixel_indices\n",
    "\n",
    "    def prepare_input(self, mlDataOutput, pixel_indices, mask=None):\n",
    "        ddir = mlDataOutput[\"dir\"]+0\n",
    "        return self.prepare_input_(ddir, pixel_indices, mask)\n",
    "\n",
    "\n",
    "# Assuming scene_cfg, gparams, ml_data, and other necessary variables are properly defined\n",
    "scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "# Initialize NIRC_Model\n",
    "nirc_eqmem_model = NIRC_EQMem_Model(init_method='uniform_large', config_path=\"data/nirc_equal_mem_dummy.json\", width=gparams.render_width, height=gparams.render_height, params_per_pixel=74, device=device_t)\n",
    "nirc_eqmem_model.set_checkpoint_path(scene_cfg.checkpoint_dir)\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "start_epoch, _ = nirc_eqmem_model.load_checkpoint(skip=False)\n",
    "\n",
    "# Training parameters\n",
    "N = scene_cfg.epochs  # number of epochs\n",
    "B = 4  # batch size\n",
    "simulate_tiles = False  # like in the original NRC work\n",
    "tile_width, tile_height = 8, 8  # Sampling one point per 2x2 tile\n",
    "eps = 0.005\n",
    "lambda_l2 = 0.00000001  # Weight for L2 regularization\n",
    "L2Reg = False\n",
    "\n",
    "# Enter training mode\n",
    "nirc_eqmem_model.train()\n",
    "\n",
    "re_accum = 1\n",
    "\n",
    "onlyRender = False\n",
    "if start_epoch >= N:\n",
    "    if gparams.RECOMPUTE_LOSS[\"NIRC_EM\"]:\n",
    "        start_epoch = N\n",
    "        nirc_eqmem_model.rL2 = 0\n",
    "    else:\n",
    "        onlyRender = True\n",
    "\n",
    "\n",
    "if start_epoch == 0:\n",
    "    nirc_eqmem_model.eval()\n",
    "    for i in range(50):\n",
    "        trainFrame()\n",
    "        mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "        # Prepare inputs\n",
    "        y = 0.0+mlDataOutput[\"radiance\"]  # Clone to manage memory\n",
    "        y = torch.clamp(y, -10000, 10000)\n",
    "        thp = mlDataOutput[\"thp\"]+0.000\n",
    "        arr = torch.arange(thp.shape[0])\n",
    "        pixel_indices = torch.stack((arr % gparams.render_width, arr // gparams.render_width), dim=1).to(device_t)\n",
    "        X, pixel_indices= nirc_eqmem_model.prepare_input(mlDataOutput, pixel_indices)\n",
    "        res = nirc_eqmem_model(X,pixel_indices,  thp).detach()        \n",
    "        bias = res-y\n",
    "        nirc_eqmem_model.variance_helper.update_ema(indices=None, bias=bias)\n",
    "        print(f\"Initial Variance Estimation {i}, rVar: {nirc_eqmem_model.variance_helper.get_variance_ema()}\")\n",
    "        \n",
    "    nirc_eqmem_model.train()\n",
    "\n",
    "for epoch in range(start_epoch, N + scene_cfg.var_est_steps):\n",
    "    start_render = time.time()\n",
    "    # Render a new frame each epoch\n",
    "    trainFrame()\n",
    "    # Get the newly rendered data\n",
    "    mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "    render_time = time.time() - start_render\n",
    "\n",
    "\n",
    "    # Generate pixel indices\n",
    "    arr = torch.arange(mlDataOutput[\"dir\"].shape[0])\n",
    "    pixel_indices = torch.stack((arr % gparams.render_width, arr // gparams.render_width), dim=1).to(device_t)\n",
    "\n",
    "    # Prepare inputs\n",
    "    y = mlDataOutput[\"radiance\"]+0.0  # Clone to manage memory\n",
    "    thp = mlDataOutput[\"thp\"]+0.0\n",
    "    pdf = safe_pdf(mlDataOutput[\"pdf\"]+0.0)\n",
    "    # y = y*thp\n",
    "    X, pixel_indices = nirc_eqmem_model.prepare_input(mlDataOutput, pixel_indices)\n",
    "\n",
    "    width, height = gparams.render_width, gparams.render_height\n",
    "    total_samples = X.size()[0]\n",
    "    samples_per_batch = total_samples // B\n",
    "\n",
    "        \n",
    "    loss_epoch = epoch-N\n",
    "    bTrainNow = loss_epoch < 0\n",
    "    for batch_index in range(B):\n",
    "        if onlyRender:\n",
    "            continue\n",
    "\n",
    "        if bTrainNow:\n",
    "            nirc_eqmem_model.optimizer.zero_grad()\n",
    "\n",
    "        # Step 1: Obtain initial batch indices\n",
    "        batch_indices = vectorized_uniform_stratified_sampling(width, height, tile_width, tile_height)\n",
    "\n",
    "        # Step 2: Fetch batch_y to compute the mask\n",
    "        batch_y = y[batch_indices]\n",
    "\n",
    "        # Step 3: Compute the mask based on batch_y\n",
    "        mask = get_mask_for_training(batch_y, False)\n",
    "\n",
    "        # Step 4: Update the indices based on the mask\n",
    "        batch_indices = batch_indices[mask]\n",
    "\n",
    "        # Step 5: Fetch all the data for the batch using updated indices\n",
    "        batch_x = X[batch_indices]\n",
    "        batch_y = y[batch_indices]\n",
    "        batch_thp = thp[batch_indices]\n",
    "        batch_pdf = pdf[batch_indices]\n",
    "        batch_pixel_indices = pixel_indices[batch_indices]\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nirc_eqmem_model(batch_x, batch_pixel_indices, batch_thp)\n",
    "\n",
    "        if not bTrainNow:\n",
    "            outputs = outputs.detach()\n",
    "            loss_val = 0\n",
    "\n",
    "        loss = relativeL2(outputs, batch_y, pdf=batch_pdf, div=ref_estimations[batch_indices])\n",
    "\n",
    "        if True:\n",
    "            batch_y = torch.clamp(batch_y, -10000, 10000)\n",
    "            bias = outputs.detach()-batch_y\n",
    "            if not bTrainNow:\n",
    "                nirc_eqmem_model.variance_helper.update(indices=batch_indices, bias=bias)\n",
    "            else:\n",
    "                nirc_eqmem_model.variance_helper.update_ema(bias, indices=batch_indices)\n",
    "\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        rL2 = loss.item()\n",
    "        if bTrainNow:\n",
    "            if epoch == 0 and batch_index == 0:\n",
    "                nirc_eqmem_model.rL2 = rL2\n",
    "            nirc_eqmem_model.rL2 = rL2*0.90+nirc_eqmem_model.rL2*0.10\n",
    "        else:\n",
    "            overe = 1.0 / re_accum\n",
    "            nirc_eqmem_model.rL2 = rL2* overe + (1-overe) * nirc_eqmem_model.rL2\n",
    "            re_accum += 1\n",
    "\n",
    "        # Backward and optimize\n",
    "        if bTrainNow:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(nirc_eqmem_model.model.parameters(), 3000)  # Clipping gradients to avoid explosion\n",
    "            nirc_eqmem_model.optimizer.step()\n",
    "    nirc_eqmem_model.scheduler.step()\n",
    "    # Reporting\n",
    "    if epoch == start_epoch:\n",
    "        average_loss = loss_val\n",
    "    else:\n",
    "        alpha = 0.95\n",
    "        average_loss = average_loss * alpha + (1.0 - alpha) * loss_val\n",
    "\n",
    "    if bTrainNow:\n",
    "        var = nirc_eqmem_model.variance_helper.get_variance_ema(epoch=epoch)\n",
    "    else:\n",
    "        var = nirc_eqmem_model.variance_helper.get_variance()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or onlyRender:\n",
    "        print(f'Epoch [{epoch + 1}/{N}], AveragedLoss: {average_loss}, Loss: {loss_val}, rL2_loss: {nirc_eqmem_model.rL2} variance: {var}')\n",
    "\n",
    "    if (loss_val == 0 or math.isinf(loss_val) or math.isnan(loss_val)) and bTrainNow:\n",
    "        start_epoch, _ = nirc_eqmem_model.load_checkpoint(skip=False)\n",
    "        epoch = start_epoch\n",
    "        print(\"RESTART!\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "        # Define the conditions and intervals as tuples (max_epoch, interval)\n",
    "    conditions = [\n",
    "        (10, 10),\n",
    "        (25, 20),\n",
    "        (500, 40),\n",
    "    ]\n",
    "    # Example usage in training loop\n",
    "    if (should_visualize(epoch, conditions) and bTrainNow) or loss_epoch == 0 or onlyRender:\n",
    "        #nirc_eqmem_model.eval()\n",
    "        visualize_model_output((nirc_eqmem_model(X, pixel_indices, thp).detach()).cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "        #nirc_eqmem_model.train()\n",
    "\n",
    "    if (epoch) % 100 == 99:\n",
    "        nirc_eqmem_model.save_checkpoint(epoch, average_loss)\n",
    "    \n",
    "    if onlyRender:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHModel(torch.nn.Module):\n",
    "    def __init__(self, num_bands, width, height):\n",
    "        super(SHModel, self).__init__()\n",
    "        self.num_bands = num_bands\n",
    "        self.num_coeffs = self.num_bands**2  # Total number of SH coefficients for n bands\n",
    "        self.sh_coefficients = torch.nn.Parameter(torch.zeros(3, width * height, self.num_coeffs))  # 3 sets of coefficients for RGB channels\n",
    "        self.rL2 = 0\n",
    "\n",
    "    def forward(self, dirs, debugID=None, mask=None):\n",
    "        batch_size = dirs.size(0)\n",
    "        dirs = dirs\n",
    "        norm = (torch.sqrt(torch.sum(dirs**2, dim=1))).unsqueeze(1)\n",
    "        dirs = dirs / norm\n",
    "\n",
    "        radiance = torch.zeros(batch_size, 3, device=dirs.device)\n",
    "\n",
    "        for i in range(3):  # Process each color channel\n",
    "            if debugID is not None:\n",
    "                # Use only the coefficients corresponding to the debugID\n",
    "                coefficients = self.sh_coefficients[i, debugID, :].unsqueeze(0).repeat(batch_size, 1)\n",
    "            else:\n",
    "                coefficients = self.sh_coefficients[i]\n",
    "\n",
    "            if mask is not None:\n",
    "                coefficients = coefficients[mask]\n",
    "            radiance[:, i] = self.eval_sh(dirs, coefficients)\n",
    "\n",
    "        return radiance\n",
    "\n",
    "    def set_checkpoint_path(self, checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(checkpoint_dir, f'sh_{self.num_coeffs * 3}_checkpoint.pth')\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        if self.checkpoint_path:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'num_bands': self.num_bands,\n",
    "                'sh_coefficients': self.state_dict(),\n",
    "                'loss': loss,\n",
    "                'rL2': self.rL2,\n",
    "            }, self.checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, skip=False):\n",
    "        if not skip and self.checkpoint_path and os.path.exists(self.checkpoint_path):\n",
    "            checkpoint = torch.load(self.checkpoint_path)\n",
    "\n",
    "            self.num_bands = checkpoint['num_bands']\n",
    "            self.num_coeffs = self.num_bands**2\n",
    "            self.load_state_dict(checkpoint['sh_coefficients'])\n",
    "            epoch = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "            self.rL2 = checkpoint['rL2']\n",
    "\n",
    "            print(f\"Checkpoint loaded: Epoch {epoch}, Loss {self.rL2}\")\n",
    "            return epoch, loss\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "    def eval_sh(self, dirs, coefficients):\n",
    "        # Compute SH basis functions up to the given degree\n",
    "        x, y, z = dirs[:, 0], dirs[:, 1], dirs[:, 2]\n",
    "        batch_size = dirs.size(0)\n",
    "        data_out = torch.zeros(batch_size, device=dirs.device)\n",
    "\n",
    "        # Compute necessary products and powers\n",
    "        xy = x * y\n",
    "        xz = x * z\n",
    "        yz = y * z\n",
    "        x2 = x * x\n",
    "        y2 = y * y\n",
    "        z2 = z * z\n",
    "\n",
    "        # Include terms for each band, similar to the provided eval_sh function\n",
    "        # Band 0\n",
    "        xy = x * y\n",
    "        xz = x * z\n",
    "        yz = y * z\n",
    "        x2 = x * x\n",
    "        y2 = y * y\n",
    "        z2 = z * z\n",
    "        x4 = x2 * x2\n",
    "        y4 = y2 * y2\n",
    "        z4 = z2 * z2\n",
    "\n",
    "        # First band: 0\n",
    "        data_out += 0.28209479177387814 * coefficients[:, 0]\n",
    "\n",
    "        # Second band: 1\n",
    "        if self.num_bands > 1:\n",
    "            data_out += -0.48860251190291987 * y * coefficients[:, 1]\n",
    "            data_out += 0.48860251190291987 * z * coefficients[:, 2]\n",
    "            data_out += -0.48860251190291987 * x * coefficients[:, 3]\n",
    "\n",
    "        # Third band: 2\n",
    "        if self.num_bands > 2:\n",
    "            data_out += 1.0925484305920792 * xy * coefficients[:, 4]\n",
    "            data_out += -1.0925484305920792 * yz * coefficients[:, 5]\n",
    "            data_out += (0.94617469575755997 * z2 - 0.31539156525251999) * coefficients[:, 6]\n",
    "            data_out += -1.0925484305920792 * xz * coefficients[:, 7]\n",
    "            data_out += (0.54627421529603959 * (x2 - y2)) * coefficients[:, 8]\n",
    "\n",
    "        # Fourth band: 3\n",
    "        if self.num_bands > 3:\n",
    "            data_out += 0.59004358992664352 * y * (-3 * x2 + y2) * coefficients[:, 9]\n",
    "            data_out += 2.8906114426405538 * xy * z * coefficients[:, 10]\n",
    "            data_out += 0.45704579946446572 * y * (1 - 5 * z2) * coefficients[:, 11]\n",
    "            data_out += 0.3731763325901154 * z * (5 * z2 - 3) * coefficients[:, 12]\n",
    "            data_out += 0.45704579946446572 * x * (1 - 5 * z2) * coefficients[:, 13]\n",
    "            data_out += 1.4453057213202769 * z * (x2 - y2) * coefficients[:, 14]\n",
    "            data_out += 0.59004358992664352 * x * (-x2 + 3 * y2) * coefficients[:, 15]\n",
    "\n",
    "        # Fifth band: 4\n",
    "        if self.num_bands > 4:\n",
    "            data_out += 2.5033429417967046 * xy * (x2 - y2) * coefficients[:, 16]\n",
    "            data_out += 1.7701307697799304 * yz * (-3 * x2 + y2) * coefficients[:, 17]\n",
    "            data_out += 0.94617469575756008 * xy * (7 * z2 - 1) * coefficients[:, 18]\n",
    "            data_out += 0.66904654355728921 * yz * (3 - 7 * z2) * coefficients[:, 19]\n",
    "            data_out += (-3.1735664074561294 * z2 + 3.7024941420321507 * z4 + 0.31735664074561293) * coefficients[:, 20]\n",
    "            data_out += 0.66904654355728921 * xz * (3 - 7 * z2) * coefficients[:, 21]\n",
    "            data_out += 0.47308734787878004 * (x2 - y2) * (7 * z2 - 1) * coefficients[:, 22]\n",
    "            data_out += 1.7701307697799304 * xz * (-x2 + 3 * y2) * coefficients[:, 23]\n",
    "            data_out += (-3.7550144126950569 * x2 * y2 + 0.62583573544917614 * x4 + 0.62583573544917614 * y4) * coefficients[:, 24]\n",
    "\n",
    "        return data_out\n",
    "    \n",
    "\n",
    "import time\n",
    "\n",
    "scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "# how many bands?\n",
    "sh_model = SHModel(num_bands=5, width=gparams.render_width, height=gparams.render_height).to(device_t)\n",
    "optimizer = torch.optim.Adam(sh_model.parameters(), lr=gparams.ADAM_LR*scene_cfg.lr_factor, betas=gparams.ADAM_BETAS)\n",
    "\n",
    "sh_model.set_checkpoint_path(scene_cfg.checkpoint_dir)\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "start_epoch, _ = sh_model.load_checkpoint(skip=False)\n",
    "\n",
    "relative_error = False\n",
    "N = 1000  # number of epochs\n",
    "variance_helper = VarianceHelper(gparams.render_width, gparams.render_height, device=device_t)\n",
    "\n",
    "# we use gradient backprogatation rather than just plain SH projection because it converges faster (the power of Adam)\n",
    "onlyRender = False\n",
    "if start_epoch >= N:\n",
    "    if gparams.RECOMPUTE_LOSS[\"SH\"]:\n",
    "        start_epoch = N\n",
    "        sh_model.rL2 = 0\n",
    "    else:\n",
    "        onlyRender = True\n",
    "\n",
    "re_accum = 1\n",
    "\n",
    "\n",
    "sum_c = 1\n",
    "sumrender = None\n",
    "for epoch in range(start_epoch, N + gparams.scene_cgf.var_est_steps):\n",
    "    loss_epoch = epoch-N\n",
    "    bTrainNow = loss_epoch < 0\n",
    "    if not bTrainNow:\n",
    "        sh_model.eval()\n",
    "    else:\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "    start_render = time.time()\n",
    "    # Render a new frame each epoch\n",
    "    trainFrame()\n",
    "\n",
    "\n",
    "    # Get the newly rendered data\n",
    "    mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "\n",
    "    y = mlDataOutput[\"radiance\"]+0.0\n",
    "    thp = mlDataOutput[\"thp\"]+0.0\n",
    "    pdf = safe_pdf(mlDataOutput[\"pdf\"]+0.0)\n",
    "    \n",
    "\n",
    "    thp_original = thp\n",
    "    pdf_original = pdf\n",
    "    pdf_original = pdf\n",
    "    dirs = mlDataOutput[\"dir\"]+0.0\n",
    "    dirs_original = dirs\n",
    "\n",
    "    # y = y*thp\n",
    "\n",
    "    mask = get_mask_for_training(y)\n",
    "    mask = mask\n",
    "    mask = mask \n",
    "    # Filter out NaNs from 'y' and corresponding elements in 'X'\n",
    "\n",
    "    num_elements = pdf.shape[0]\n",
    "    y = y[mask]\n",
    "    thp = thp[mask]\n",
    "    dirs = dirs[mask]\n",
    "    pdf = pdf[mask]\n",
    "\n",
    "    num_used_elements = pdf.shape[0]\n",
    "\n",
    "    # Prepare the data  \n",
    "    loss_val = 0\n",
    "\n",
    "    # Forward pass\n",
    "    if not onlyRender:\n",
    "        outputs = sh_model(dirs, mask=mask)\n",
    "\n",
    "        if not bTrainNow:\n",
    "            outputs = outputs.detach()\n",
    "        \n",
    "\n",
    "        mcestimator = outputs/pdf\n",
    "        loss = relativeL2(mcestimator, y, pdf, div=ref_estimations[mask])\n",
    "        rL2 = loss.detach()\n",
    "        #loss = torch.mean((outputs/pdf-y*thp)**2)\n",
    "        #loss = relativeL2PDFCounted(outputs, y*pdf, pdf=pdf, div=ref_estimations)\n",
    "        #rL2 = relativeL2PDFCounted(outputs.detach(), y*pdf, pdf=pdf, div=ref_estimations)\n",
    "        #loss = rL2\n",
    "        \n",
    "        \n",
    "        loss_val = loss.item()\n",
    "        rL2 = rL2.item()\n",
    "        if bTrainNow:\n",
    "            if epoch == 0:\n",
    "                sh_model.rL2 = rL2\n",
    "            sh_model.rL2 = rL2*0.90+sh_model.rL2*0.10\n",
    "        else:\n",
    "            bias = mcestimator.detach()-y\n",
    "            #variance_helper.update(mask, bias)\n",
    "\n",
    "            overe = 1.0 / re_accum\n",
    "            sh_model.rL2 = rL2* overe + (1-overe) * sh_model.rL2\n",
    "            re_accum += 1\n",
    "        \n",
    "        # Backward and optimize\n",
    "        start_backprop = time.time()\n",
    "        \n",
    "        \n",
    "        # Backward and optimize\n",
    "        if bTrainNow:\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(sh_model.parameters(), 1000)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    if epoch == 0:\n",
    "        average_loss = loss_val\n",
    "    else:\n",
    "        alpha = 0.95\n",
    "        average_loss = average_loss*alpha+(1.0-alpha)*loss_val\n",
    "         \n",
    "         \n",
    "    if (epoch+1) % 2 == 0 or onlyRender:\n",
    "        print(f'Epoch [{epoch+1}/{N}],  AverageLoss: {average_loss}, Loss: {loss_val}, rL2_loss: {sh_model.rL2} Num Used Elements {num_used_elements}/{num_elements} Variance {variance_helper.get_variance()}')\n",
    "    \n",
    "    if (epoch) % 100 == 99:\n",
    "        sh_model.save_checkpoint(epoch, average_loss)\n",
    "\n",
    "    conditions = [\n",
    "        (10, 2),\n",
    "        (25, 5),\n",
    "        (50, 20),\n",
    "        (1000, 50),\n",
    "    ]\n",
    "\n",
    "    if (should_visualize(epoch, conditions) and bTrainNow) or epoch == N or onlyRender:\n",
    "        sh_model.eval()\n",
    "\n",
    "        p = sh_model(dirs_original)/pdf_original\n",
    "\n",
    "        if sum_c == 1:\n",
    "            sumrender = p\n",
    "            sum_c += 1\n",
    "        else:\n",
    "            w = 1.0/sum_c\n",
    "            sumrender = p*w + sumrender*(1-w)\n",
    "            sum_c += 1\n",
    "        visualize_model_output((p).cpu(), gparams.render_width, gparams.render_height, epoch)\n",
    "        sh_model.train()\n",
    "\n",
    "    if onlyRender:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(values, dim=1):\n",
    "    # Subtract the maximum value to prevent overflow or underflow\n",
    "    max_val = torch.max(values, dim=dim, keepdim=True)[0]\n",
    "    stable_logsumexp = max_val + torch.log(torch.sum(torch.exp(values - max_val), dim=dim, keepdim=True))\n",
    "    return stable_logsumexp\n",
    "\n",
    "class VMFModel(torch.nn.Module):\n",
    "    def __init__(self, width, height, K, alpha = 0.8, max_kappa=200,min_kappa=None, device=None, kappa_force=None, debug_amplitudes=True, normals=None, em_delay=0, batch_size=1):\n",
    "        super(VMFModel, self).__init__()\n",
    "        self.K = K\n",
    "        self.max_kappa = max_kappa\n",
    "        self.min_kappa = min_kappa\n",
    "        self.num_pixels = width * height\n",
    "        self.alpha = alpha\n",
    "        self.em_delay = em_delay\n",
    "        self.batch_size = 10\n",
    "        self.device = device if device is not None else torch.device(\"cuda\")\n",
    "        self.params_per_lobe = 8\n",
    "        self.vmf_coefficients = torch.zeros(self.num_pixels, K * self.params_per_lobe, device=self.device)\n",
    "        self.S_data = torch.zeros((K, self.num_pixels, 3), device=self.device)\n",
    "        self.S_weight_accum = torch.zeros((K, self.num_pixels, 1), device=self.device)\n",
    "        self.stats_point_weight = torch.zeros((self.num_pixels, 1), device=self.device)\n",
    "        self.accumulated_lum = torch.zeros((self.num_pixels, 1), device=self.device)\n",
    "        self.amplitudes = torch.nn.Parameter(torch.ones((self.num_pixels, K, 3), device=device_t))\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=gparams.ADAM_LR, betas=gparams.ADAM_BETAS)\n",
    "\n",
    "        self.init_params(kappa_force, debug_amplitudes=debug_amplitudes, normals=normals)\n",
    "        self.rL2 = 0\n",
    "\n",
    "        self.params_per_pixel = K * (self.params_per_lobe - 1)\n",
    "    \n",
    "    def set_checkpoint_path(self, checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(checkpoint_dir, f'vmf_{self.params_per_pixel}_checkpoint.pth')\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        self.K = state['K']\n",
    "        self.max_kappa = state['max_kappa']\n",
    "        self.min_kappa = state['min_kappa']\n",
    "        self.num_pixels = state['num_pixels']\n",
    "        self.alpha = state['alpha']\n",
    "        self.em_delay = state['em_delay']\n",
    "        self.batch_size = state['batch_size']\n",
    "        self.device = state['device']\n",
    "        self.params_per_lobe = state['params_per_lobe']\n",
    "        self.vmf_coefficients = state['vmf_coefficients']\n",
    "        self.S_data = state['S_data']\n",
    "        self.amplitudes.data = state['amplitudes']  # Load amplitudes\n",
    "        self.S_weight_accum = state['S_weight_accum']\n",
    "        self.stats_point_weight = state['stats_point_weight']\n",
    "        self.accumulated_lum = state['accumulated_lum']\n",
    "        self.params_per_pixel = state['params_per_pixel']\n",
    "        super().load_state_dict(state['model_state'])\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = {\n",
    "            'K': self.K,\n",
    "            'max_kappa': self.max_kappa,\n",
    "            'min_kappa': self.min_kappa,\n",
    "            'num_pixels': self.num_pixels,\n",
    "            'alpha': self.alpha,\n",
    "            'em_delay': self.em_delay,\n",
    "            'batch_size': self.batch_size,\n",
    "            'device': self.device,\n",
    "            'params_per_lobe': self.params_per_lobe,\n",
    "            'vmf_coefficients': self.vmf_coefficients,\n",
    "            'S_data': self.S_data,\n",
    "            'amplitudes': self.amplitudes.data,  # Include amplitudes\n",
    "            'S_weight_accum': self.S_weight_accum,\n",
    "            'stats_point_weight': self.stats_point_weight,\n",
    "            'accumulated_lum': self.accumulated_lum,\n",
    "            'params_per_pixel': self.params_per_pixel,\n",
    "            'model_state': super().state_dict()  # Save the state of the nn.Module\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        if self.checkpoint_path:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.state_dict(),  # Use the updated state_dict\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'rL2': self.rL2,\n",
    "            }, self.checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, skip=False):\n",
    "        if not skip and self.checkpoint_path and os.path.exists(self.checkpoint_path):\n",
    "            print(self.checkpoint_path)\n",
    "            checkpoint = torch.load(self.checkpoint_path)\n",
    "            self.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            epoch = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "            self.rL2 = checkpoint['rL2']\n",
    "\n",
    "            print(f\"Checkpoint loaded: Epoch {epoch}, Loss {loss}\")\n",
    "            return epoch, loss\n",
    "        else:\n",
    "            return 0, None\n",
    "\n",
    "    def init_params(self, kappa_force=None, debug_amplitudes=False, normals=None):\n",
    "        # Fibonacci sphere sampling for mean direction initialization\n",
    "        golden_ratio = (1 + 5 ** 0.5) / 2\n",
    "        indices = torch.arange(0, self.K, dtype=torch.float32)\n",
    "        theta = 2 * np.pi * indices / golden_ratio\n",
    "        phi = torch.acos(1 - 2 * (indices + 0.5) / self.K)\n",
    "        x = torch.cos(theta) * torch.sin(phi)\n",
    "        y = torch.sin(theta) * torch.sin(phi)\n",
    "        z = torch.cos(phi)\n",
    "\n",
    "        mean_directions_init = torch.stack([x, y, z], dim=1).to(self.device)\n",
    "        \n",
    "        # Compute initial sharpness for kappa values based on direction pairs\n",
    "        sharpness = self.calculate_kappa(kappa_force, mean_directions_init)\n",
    "        if self.min_kappa is None:\n",
    "            self.min_kappa = sharpness\n",
    "\n",
    "        mean_directions_init = mean_directions_init.unsqueeze(0).repeat(self.num_pixels, 1, 1)\n",
    "        print(mean_directions_init.shape)\n",
    "        if normals is not None:\n",
    "            # Assume normals are normalized and have shape (num_pixels, 3)\n",
    "            mean_directions_init = self.reproject_to_tangent_space(normals, mean_directions_init)\n",
    "\n",
    "\n",
    "        print(f\"Kappa is init with value: {sharpness} for the {self.K} vMF lobes\")\n",
    "\n",
    "        # Initialize vmf_coefficients with new sharpness values\n",
    "        for k in range(self.K):\n",
    "            start = k * self.params_per_lobe\n",
    "            self.vmf_coefficients[:, start:start+3] = mean_directions_init[:, k]\n",
    "            self.vmf_coefficients[:, start+3] = torch.full((self.num_pixels,), sharpness, device=self.device)\n",
    "            self.vmf_coefficients[:, start+4] = 1.0 / self.K\n",
    "\n",
    "        # Debug amplitude initialization\n",
    "        if debug_amplitudes:\n",
    "            self.amplitudes.data = self.generate_colors(self.K, self.num_pixels, self.device)\n",
    "\n",
    "    def calculate_kappa(self, kappa_force, mean_directions):\n",
    "        if kappa_force is not None:\n",
    "            return kappa_force\n",
    "        minDP = 1.0\n",
    "        for i in range(1, self.K):\n",
    "            h = (mean_directions[i] + mean_directions[0])\n",
    "            h = h / (h.norm(dim=-1, keepdim=True) + 0.00001)\n",
    "            minDP = min(minDP, torch.dot(h, mean_directions[0]).item())\n",
    "        if self.K == 1:\n",
    "            return torch.tensor(1.0, device=self.device)\n",
    "        else:\n",
    "            return (torch.log(torch.tensor(0.65, device=self.device)) * self.K) / (minDP - 1.0001)\n",
    "\n",
    "    def reproject_to_tangent_space(self, normals, directions):\n",
    "        # Create local coordinate systems\n",
    "        up = torch.tensor([0.0, 0.0, 1.0], device=self.device).expand_as(normals)\n",
    "        tangent = torch.cross(normals, up)\n",
    "        tangent = tangent / (tangent.norm(dim=1, keepdim=True) + 1e-10)  # Normalize\n",
    "        bitangent = torch.cross(normals, tangent)\n",
    "        bitangent = bitangent / (bitangent.norm(dim=1, keepdim=True) + 1e-10)\n",
    "\n",
    "        # Expand directions to match each pixel\n",
    "        expanded_directions = directions\n",
    "\n",
    "        # Initialize the container for reprojected directions\n",
    "        reprojected_directions = torch.zeros_like(expanded_directions)  # [num_pixels, K, 3]\n",
    "\n",
    "        # Form the rotation matrix for each normal\n",
    "        rotation_matrix = torch.stack([tangent, bitangent, normals], dim=-1)  # [num_pixels, 3, 3]\n",
    "\n",
    "        # Reproject each lobe's directions\n",
    "        for k in range(self.K):\n",
    "            direction_k = expanded_directions[:, k, :]  # Now correctly indexing [num_pixels, 3]\n",
    "            \n",
    "            # Apply the rotation to the k-th direction vector\n",
    "            reprojected_dir_k = torch.bmm(rotation_matrix, direction_k.unsqueeze(-1)).squeeze(-1)  # [num_pixels, 3]\n",
    "            \n",
    "            # Ensure the reprojected direction is in the upper hemisphere relative to the normals\n",
    "            dot_products = torch.sum(reprojected_dir_k * normals, dim=1, keepdim=True)\n",
    "            mask = dot_products < 0\n",
    "            reprojected_dir_k[mask.squeeze(1)] *= -1\n",
    "\n",
    "            # Store the reprojected direction back into the container\n",
    "            reprojected_directions[:, k, :] = reprojected_dir_k\n",
    "\n",
    "        return reprojected_directions\n",
    "\n",
    "\n",
    "    def generate_colors(self, K, num_pixels, device):\n",
    "        # Generate distinct colors by varying the hue in HSV space\n",
    "        hues = torch.linspace(0, 1 - 1e-6, steps=K)  # Small epsilon to simulate endpoint=False\n",
    "        colors = torch.zeros((K, 3), device=device)\n",
    "        for i, hue in enumerate(hues):\n",
    "            colors[i] = torch.tensor(self.hsv_to_rgb(hue, 1.0, 1.0))  # Saturation and Value are set to 1 for vibrant colors\n",
    "        return colors.unsqueeze(1).expand(-1, num_pixels, -1).permute(1, 0, 2)  # Shape: (num_pixels, K, 3)\n",
    "\n",
    "    def hsv_to_rgb(self, h, s, v):\n",
    "        i = int(h * 6.)\n",
    "        f = h * 6. - i\n",
    "        p = v * (1. - s)\n",
    "        q = v * (1. - f * s)\n",
    "        t = v * (1. - (1. - f) * s)\n",
    "        i = i % 6\n",
    "        if i == 0:\n",
    "            return [v, t, p]\n",
    "        if i == 1:\n",
    "            return [q, v, p]\n",
    "        if i == 2:\n",
    "            return [p, v, t]\n",
    "        if i == 3:\n",
    "            return [p, q, v]\n",
    "        if i == 4:\n",
    "            return [t, p, v]\n",
    "        if i == 5:\n",
    "            return [v, p, q]\n",
    "\n",
    "    def forward(self, dirs, mask=None, debugID=None):\n",
    "        return self.compute_radiance(dirs, mask, debugID)\n",
    "\n",
    "    def compute_radiance(self, dirs, mask = None, debugID = None):\n",
    "        num_pixels = dirs.shape[0]\n",
    "        radiance = torch.zeros((num_pixels, 3), device=self.device)\n",
    "        vmfs_ = self.vmf_coefficients\n",
    "        amplitudes_ = self.amplitudes\n",
    "\n",
    "        if mask != None:\n",
    "            vmfs_ = vmfs_[mask]\n",
    "            amplitudes_ = amplitudes_[mask]\n",
    "\n",
    "        if debugID is not None:\n",
    "            vmfs_ = self.vmf_coefficients[debugID].unsqueeze(0).repeat(num_pixels, 1)\n",
    "            amplitudes_ = self.amplitudes[debugID].unsqueeze(0).repeat(num_pixels, 1, 1)\n",
    "\n",
    "\n",
    "        for k in range(self.K):\n",
    "            start_index = k * self.params_per_lobe\n",
    "            mu = vmfs_[:, start_index:start_index+3]\n",
    "            kappa = vmfs_[:, start_index+3]\n",
    "            weight = vmfs_[:, start_index+4]\n",
    "            mu = torch.nn.functional.normalize(mu, dim=1)\n",
    "            amplitudes = amplitudes_[:, k]\n",
    "            cos_theta = torch.einsum('ij,ij->i', dirs, mu)\n",
    "            pdf_constant = kappa / (2 * torch.pi * (1.0 - torch.exp(-2.0 * kappa)))\n",
    "            pdf_values = pdf_constant * torch.exp(kappa * (cos_theta - 1))\n",
    "            amplitudes = torch.relu(amplitudes)\n",
    "            assert(check_tensor(weight, \"weight\"))\n",
    "            assert(check_tensor(pdf_values, \"pdf_values\"))\n",
    "            assert(check_tensor(amplitudes, \"amplitudes\"))\n",
    "            radiance += weight.unsqueeze(1) * pdf_values.unsqueeze(1) *amplitudes   # Assuming mu as amplitude\n",
    "        return radiance\n",
    "\n",
    "    def calculate_responsibilities(self, dirs, y_lum):\n",
    "        num_pixels = dirs.shape[0]\n",
    "        mus = torch.zeros((num_pixels, self.K, 3), device=dirs.device, dtype=dirs.dtype)\n",
    "        kappas = torch.zeros((num_pixels, self.K), device=dirs.device, dtype=dirs.dtype)\n",
    "        weights = torch.zeros((num_pixels, self.K), device=dirs.device, dtype=dirs.dtype)\n",
    "        for k in range(self.K):\n",
    "            start_idx = k * self.params_per_lobe\n",
    "            mus[:, k, :] = self.vmf_coefficients[:, start_idx:start_idx+3]\n",
    "            kappas[:, k] = self.vmf_coefficients[:, start_idx+3]\n",
    "            weights[:, k] = self.vmf_coefficients[:, start_idx+4]\n",
    "\n",
    "\n",
    "        if not gparams.SKIP_TENSOR_CHECK:\n",
    "            row_sums = torch.sum(weights, dim=1)\n",
    "            if not torch.allclose(row_sums, torch.ones_like(row_sums)):\n",
    "                assert(check_tensor(responsibilities, \"responsibilities\", True))\n",
    "                print(\"responsibilities Normalization check failed.\")\n",
    "                print(\"Row sums:\", row_sums)\n",
    "                assert(0)\n",
    "        \n",
    "        weights_sum = torch.sum(weights, dim=1)\n",
    "        if not torch.allclose(weights_sum, torch.ones_like(weights_sum)):\n",
    "            assert(check_tensor(weights_sum, \"weights_sum\", True))\n",
    "            print(\"weights Normalization check failed.\")\n",
    "            print(\"Row sums:\", weights_sum)\n",
    "            assert(0)\n",
    "\n",
    "        dot_products = torch.einsum('ijk,ik->ij', mus, dirs)\n",
    "\n",
    "        pdf_constant = kappas / (2 * torch.pi * (1.0 - torch.exp(-2.0 * kappas)))\n",
    "\n",
    "        pdf_values = pdf_constant * torch.exp(kappas * (dot_products - 1))\n",
    "\n",
    "\n",
    "        pdf_values = pdf_values+1e-8\n",
    "        responsibilities = weights * pdf_values\n",
    "        responsibilities = responsibilities+1e-10\n",
    "        log_likelihood = (torch.log(responsibilities)*y_lum).mean()\n",
    "        #assert(check_tensor(responsibilities[0], \"responsibilities_before_norm\", True))\n",
    "\n",
    "        eps = 1e-16\n",
    "        responsibilities = responsibilities/(torch.sum(responsibilities, dim=1, keepdim=True)+eps)\n",
    "        #responsibilities = torch.exp(responsibilities - log_sum_exp(responsibilities))\n",
    "\n",
    "        \n",
    "        assert(check_tensor(responsibilities, \"responsibilities\"))\n",
    "        assert(check_tensor(responsibilities, \"responsibilities\"))\n",
    "\n",
    "\n",
    "        if not gparams.SKIP_TENSOR_CHECK:\n",
    "            row_sums = torch.sum(responsibilities, dim=1)\n",
    "\n",
    "            assert(check_tensor(row_sums, \"rows_sums\"))\n",
    "            if not torch.allclose(row_sums, torch.ones_like(row_sums)):\n",
    "                assert(check_tensor(responsibilities, \"responsibilities\", True))\n",
    "                print(\"responsibilities Normalization check failed.\")\n",
    "                print(\"Row sums:\", row_sums)\n",
    "                assert(0)\n",
    "\n",
    "        assert(check_tensor(responsibilities, \"responsibilities\"))\n",
    "        return responsibilities, log_likelihood\n",
    "\n",
    "    def update_parameters(self, new_w, epoch):\n",
    "        \n",
    "        #norm = log_sum_exp(self.S_weight_accum, 0)\n",
    "        #nw = torch.exp(self.S_weight_accum - norm)\n",
    "        nw = self.S_weight_accum/torch.sum(self.S_weight_accum, dim=0, keepdim=True)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            S_data_k = self.S_data[k]\n",
    "            norm_S = torch.sqrt(torch.sum(S_data_k ** 2, dim=1, keepdim=True))\n",
    "\n",
    "            eps = 1e-18\n",
    "            new_mu = S_data_k / (norm_S + eps)\n",
    "            assert(check_tensor(new_mu, \"new_mu\"))\n",
    "            R_bar = norm_S / (self.S_weight_accum[k] + eps)\n",
    "            assert(check_tensor(R_bar, \"R_bar\"))\n",
    "            new_kappa = (R_bar * (3.0 - R_bar ** 2)) / (1.0 - R_bar ** 2 + eps)\n",
    "            assert(check_tensor(new_kappa, \"new_kappa\"))\n",
    "            new_kappa = torch.clamp(new_kappa, min=self.min_kappa, max=self.max_kappa)\n",
    "            start = k * self.params_per_lobe\n",
    "            old_weights = self.vmf_coefficients[:, start+4]\n",
    "\n",
    "\n",
    "            assert(check_tensor(new_mu, \"new_mu\"))\n",
    "\n",
    "            #new_weights = responsibilities[:, k] * weight_wo_responsibilities[:, k] + (1 - weight_wo_responsibilities[:, k]) * old_weights\n",
    "            new_mu = new_mu*new_w+(1.0-new_w)*self.vmf_coefficients[:, start:start+3]\n",
    "            new_mu = new_mu/(torch.sqrt(torch.sum(new_mu**2, dim=1, keepdim=True))+eps)\n",
    "            \n",
    "            self.vmf_coefficients[:, start:start+3] = new_mu\n",
    "            #assert(check_tensor(self.vmf_coefficients[:, start+3], \"old_kappa\", True))\n",
    "            new_kappa = new_kappa.squeeze()*new_w+self.vmf_coefficients[:, start+3]*(1.0-new_w)\n",
    "            self.vmf_coefficients[:, start+3] = new_kappa\n",
    "            #assert(check_tensor(self.vmf_coefficients[:, start+3], \"new_kappa\", True))\n",
    "            #self.vmf_coefficients[:, start+4] = nw[k].squeeze()*new_w+self.vmf_coefficients[:, start+4]*(1.0-new_w)\n",
    "\n",
    "            # Update weights using translated C++ logic\n",
    "\n",
    "            if True:\n",
    "                float_min = 1e-38  # Close to the smallest positive normal float\n",
    "                mask = self.S_weight_accum[k] > float_min\n",
    "\n",
    "                \n",
    "                num_data = epoch\n",
    "                \n",
    "                new_weights = ((self.S_weight_accum[k] / (self.stats_point_weight + eps)) * num_data + 1e-2) / (num_data + self.K * 1e-2)\n",
    "\n",
    "                \n",
    "                mask = mask.squeeze()\n",
    "                new_weights = new_weights.squeeze()\n",
    "                \n",
    "                self.vmf_coefficients[:, start+4][mask] = new_weights[mask]\n",
    "                self.vmf_coefficients[:, start+4][~mask] = self.vmf_coefficients[:, start+4][~mask]  # Maintain previous weights where mask is False\n",
    "            else:\n",
    "                self.vmf_coefficients[:, start+4] = nw[k].squeeze()*new_w+self.vmf_coefficients[:, start+4]*(1.0-new_w)\n",
    "\n",
    "\n",
    "            if not gparams.SKIP_TENSOR_CHECK and False:\n",
    "                # Check if new_mu is normalized\n",
    "                if not torch.allclose(torch.norm(new_mu, dim=1), torch.ones(new_mu.shape[0], device=new_mu.device), atol=1e-5):\n",
    "                    check_tensor(torch.norm(new_mu, dim=1), \"norm\", True)\n",
    "                    print(f\"Normalization check failed for lobe {k}. Norms of new_mu:\", torch.norm(new_mu, dim=1))\n",
    "                    assert(0)\n",
    "        \n",
    "            #self.vmf_coefficients[:, start+3] = new_kappa.squeeze()\n",
    "            #self.vmf_coefficients[:, start+4] = nw[k].squeeze()\n",
    "\n",
    "            assert(check_tensor(new_mu, \"new_mu\"))\n",
    "\n",
    "            \n",
    "            assert(check_tensor(new_kappa, \"new_kappa\"))\n",
    "\n",
    "            if epoch % 50 == 0 and False:\n",
    "                assert(check_tensor(new_kappa, \"new_kappa\", True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # After the loop, re-normalize weights to ensure they sum to 1 across all K lobes for each pixel\n",
    "        weight_sums = torch.sum(self.vmf_coefficients[:, 4::self.params_per_lobe], dim=1, keepdim=True)  # Sum weights across all K\n",
    "        self.vmf_coefficients[:, 4::self.params_per_lobe] /= weight_sums  # Normalize weights\n",
    "        assert(check_tensor(self.vmf_coefficients[:, 4::self.params_per_lobe], \"new_weights\"))\n",
    "\n",
    "    def EM_step(self, epoch, dirs, y, update_params=True):\n",
    "        assert(check_tensor(y, \"y\"))\n",
    "        y_luminance = torch.norm(y,  dim=1, keepdim=True)\n",
    "\n",
    "        \n",
    "        y_luminance += 1e-8\n",
    "        responsibilities, log_likehood = self.calculate_responsibilities(dirs, y_luminance)\n",
    "        new_w = 1.0\n",
    "        old_w = 1.0\n",
    "\n",
    "        if self.alpha != 0.0:\n",
    "            new_w = pow(epoch, -self.alpha)\n",
    "            old_w = 1.0-new_w\n",
    "        \n",
    "        if epoch != 0:\n",
    "            new_accum = y_luminance + self.accumulated_lum\n",
    "            weight = y_luminance\n",
    "\n",
    "            self.accumulated_lum = new_accum\n",
    "        else:\n",
    "            self.accumulated_lum = y_luminance\n",
    "            weight = torch.ones((self.num_pixels, 1), device=self.device) * y_luminance\n",
    "\n",
    "        self.stats_point_weight = old_w*self.stats_point_weight+new_w*weight\n",
    "        weight = weight.expand(-1, self.K)\n",
    "\n",
    "        if True:\n",
    "            weight = weight * responsibilities\n",
    "        else:\n",
    "            weight = responsibilities\n",
    "\n",
    "        assert(check_tensor(dirs, \"dirs\"))\n",
    "        assert(check_tensor(responsibilities, \"responsibilities\"))\n",
    "        assert(check_tensor(y_luminance, \"y_luminance\"))\n",
    "\n",
    "        for k in range(self.K):\n",
    "            self.S_data[k] = self.S_data[k]*old_w+dirs * weight[:, k].unsqueeze(1)*new_w\n",
    "            self.S_weight_accum[k] = self.S_weight_accum[k]*old_w+weight[:, k].unsqueeze(1)*new_w\n",
    "\n",
    "        assert(check_tensor(self.S_data, \"S_data\"))\n",
    "        assert(check_tensor(self.S_weight_accum, \"S_weight_accum\"))\n",
    "        assert(check_tensor(responsibilities, \"responsibilities\"))\n",
    "        if (epoch > self.em_delay and (epoch % self.batch_size) == 0):\n",
    "            self.update_parameters(new_w, epoch)\n",
    "        return log_likehood\n",
    "\n",
    "    def visualize_directions_for_pixel(self, pixel_id):\n",
    "        \"\"\"Visualizes the vMF directions for a given pixel ID.\"\"\"\n",
    "        if pixel_id >= self.num_pixels:\n",
    "            print(\"Pixel ID is out of bounds.\")\n",
    "            return\n",
    "\n",
    "        # Extract the direction vectors for the given pixel ID\n",
    "        directions = self.vmf_coefficients[pixel_id, :].view(self.K, self.params_per_lobe)[:, :3]  # Assuming directions are stored in the first 3 columns\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.set_ylim([-1, 1])\n",
    "        ax.set_zlim([-1, 1])\n",
    "\n",
    "        # Plot the origin\n",
    "        ax.scatter([0], [0], [0], color='red', label='Origin')\n",
    "\n",
    "        # Plot each direction vector\n",
    "        for i in range(directions.shape[0]):\n",
    "            x, y, z = directions[i].cpu().numpy()\n",
    "            ax.quiver(0, 0, 0, x, y, z, length=1.0, normalize=True)\n",
    "\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.legend()\n",
    "        plt.title(f'Direction Vectors for Pixel ID {pixel_id}')\n",
    "        plt.show()\n",
    "\n",
    "def plot_radiance(radiance, title=\"Radiance\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    radiance_np = radiance.detach().cpu().numpy()\n",
    "    plt.imshow(np.clip(radiance_np.reshape(gparams.render_height, gparams.render_width, 3), 0, 1))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "device_t = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "normals = mlDataOutput_ref[\"normal\"]\n",
    "# K = number of lobes, how many? I implemented online-em from the On-line Learning of Parametric Mixture Models for Light Transport Simulation. with the same regularization techniques, alpha value (convergence).\n",
    "# lobes are generated using the Fibonacci sequence and only in the top hemisphere of a surface. that's why we forward the normals. batch = we need it, really. as in that paper. \n",
    "model_vmf = VMFModel(gparams.render_width, gparams.render_height, K=11,max_kappa=250, alpha=0.75, device=device_t, debug_amplitudes=False, normals=normals, em_delay=20, batch_size=40)\n",
    "model_vmf.set_checkpoint_path(scene_cfg.checkpoint_dir)\n",
    "\n",
    "\n",
    "\n",
    "EM_STEPS = gparams.num_training_frames  # number of epochs\n",
    "EM_STEPS= 2500\n",
    "AMPLITUDE_DELAY = 20\n",
    "\n",
    "AMPLITUDE_ADDITIONAL_STEPS = 500\n",
    "\n",
    "Visualize = True\n",
    "\n",
    "N = EM_STEPS+AMPLITUDE_ADDITIONAL_STEPS\n",
    "start_epoch = 0\n",
    "start_epoch, _ = model_vmf.load_checkpoint(skip=False)\n",
    "scene_cfg.prepare_scene(ref_scene)\n",
    "\n",
    "isFirstWas = False\n",
    "\n",
    "estimation_rad = model_vmf(mlDataOutput[\"dir\"])\n",
    "variance_helper = VarianceHelper(gparams.render_width, gparams.render_height, device=device_t)\n",
    "\n",
    "onlyRender = False\n",
    "if start_epoch >= N:\n",
    "    if gparams.RECOMPUTE_LOSS[\"VMF\"]:\n",
    "        start_epoch = N\n",
    "        model_vmf.rL2 = 0\n",
    "    else:\n",
    "        onlyRender = True\n",
    "\n",
    "re_accum = 1\n",
    "average_loss = 0\n",
    "loss = 0\n",
    "for epoch in range(start_epoch, N + gparams.scene_cgf.var_est_steps):\n",
    "    start_render = time.time()\n",
    "    trainFrame()\n",
    "\n",
    "    # Get the newly rendered data\n",
    "    mlDataOutput = falcor_to_torch_split_interleaved(ml_data, field_types)\n",
    "\n",
    "    dirs = mlDataOutput[\"dir\"] + 0.0\n",
    "    # Find any vectors that are significantly off from length 1\n",
    "    lengths = torch.sqrt(torch.sum(dirs ** 2, dim=1, keepdim=True))\n",
    "    tolerance = 1e-5  # Set a tolerance level for floating point precision\n",
    "    \n",
    "    global incorrect_indices\n",
    "    incorrect_indices = torch.where(torch.abs(lengths - 1) > tolerance)[0]\n",
    "    \n",
    "    # (#pixels, 3)\n",
    "    y = mlDataOutput[\"radiance\"] + 0.0\n",
    "    pdf = safe_pdf(mlDataOutput[\"pdf\"]+0.0)\n",
    "    mask = get_mask_for_training(y)\n",
    "    mask = mask \n",
    "\n",
    "    y = torch.relu(y)\n",
    "\n",
    "    y[~mask] = 0\n",
    "    dirs[~mask, :] = 0.0\n",
    "    dirs[~mask, 0] = 1.0\n",
    "    loss_val = None\n",
    "    l2 = None\n",
    "    used = torch.count_nonzero(mask).item()\n",
    "    all = mask.shape[0]\n",
    "\n",
    "    loss_epoch = epoch-N\n",
    "    if epoch >= AMPLITUDE_DELAY and not onlyRender:\n",
    "        \n",
    "        \n",
    "        bTrainNow = loss_epoch < 0\n",
    "        if bTrainNow:\n",
    "            model_vmf.optimizer.zero_grad()   \n",
    "        # Forward pass\n",
    "\n",
    "        \n",
    "        estimation_rad = model_vmf(dirs)\n",
    "        \n",
    "        # Simulate a target for loss calculation\n",
    "        target = torch.randn(model_vmf.num_pixels, 3, device=device_t)\n",
    "        #inf_mask = torch.isinf(estimation_rad)\n",
    "\n",
    "\n",
    "        #estimation_rad[~mask] = 0.0\n",
    "        \n",
    "        y_masked = y[mask]\n",
    "        estimation_rad_masked = estimation_rad[mask]\n",
    "        batch_pdf = pdf[mask]\n",
    "\n",
    "        batch_y = y_masked\n",
    "        if not bTrainNow:\n",
    "            estimation_rad_masked = estimation_rad_masked.detach()\n",
    "    \n",
    "\n",
    "        if bTrainNow:\n",
    "            #loss = relativeL2_PDF_NotApplied(estimation_rad_masked, y_masked, pdf=batch_pdf, div=ref_estimations[mask])\n",
    "            #loss = torch.mean((estimation_rad_masked/batch_pdf - y_masked)**2/batch_pdf)\n",
    "            mc = estimation_rad_masked/batch_pdf\n",
    "            loss = relativeL2(mc, y_masked, pdf=batch_pdf, div=ref_estimations[mask])\n",
    "\n",
    "        rL2 = relativeL2(ref_estimations[mask], y_masked, pdf=batch_pdf, div=ref_estimations[mask]).item()\n",
    "\n",
    "        if bTrainNow:\n",
    "            total_loss = loss \n",
    "            loss_val = loss.item()\n",
    "        else:\n",
    "            loss_val = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if bTrainNow:\n",
    "            if epoch == 0:\n",
    "                model_vmf.rL2 = rL2\n",
    "            model_vmf.rL2 = rL2*0.90+model_vmf.rL2*0.10\n",
    "        else:\n",
    "            overe = 1.0 / re_accum\n",
    "            model_vmf.rL2 = rL2* overe + (1-overe) * model_vmf.rL2\n",
    "            bias = estimation_rad_masked/batch_pdf -y_masked\n",
    "            #variance_helper.update(mask, bias)\n",
    "            re_accum += 1\n",
    "    \n",
    "        # Backward and optimize\n",
    "        if bTrainNow:\n",
    "            # backpropate to optimize the Amplitude\n",
    "            loss.backward()\n",
    "\n",
    "        if epoch == AMPLITUDE_DELAY:\n",
    "            average_loss = loss\n",
    "        else:\n",
    "            alpha = 0.95\n",
    "            average_loss = average_loss*alpha+(1.0-alpha)*loss\n",
    "        \n",
    "        # Backward and optimize\n",
    "        if bTrainNow:\n",
    "            torch.nn.utils.clip_grad_norm_(model_vmf.parameters(),  5000)\n",
    "            model_vmf.optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        if (loss_val == 0 or math.isinf(loss_val) or math.isnan(loss_val)) and bTrainNow and False:\n",
    "            #print(loss_val)\n",
    "            start_epoch, _ = model_vmf.load_checkpoint(skip=False)\n",
    "            epoch = start_epoch\n",
    "            print(\"RESTART!\")\n",
    "            continue\n",
    "        \n",
    "    log_likehood = None\n",
    "    if epoch < EM_STEPS and not onlyRender:\n",
    "        log_likehood = model_vmf.EM_step(epoch+1, dirs, y).item()\n",
    "        if (epoch+1) % 100 == 0 and False:\n",
    "            model_vmf.visualize_directions_for_pixel(0)\n",
    "\n",
    "    conditions = [\n",
    "            (100, 10),\n",
    "            (25, 5),\n",
    "            (50, 50),\n",
    "    ]\n",
    "    \n",
    "    if (epoch) % 100 == 99:\n",
    "        model_vmf.save_checkpoint(epoch, average_loss)\n",
    "    if should_visualize(epoch, conditions) and (bTrainNow or not isFirstWas) and Visualize or loss_epoch == 0 or onlyRender:\n",
    "        plot_radiance(estimation_rad/pdf)\n",
    "    if (epoch+1) % 10 == 0 or onlyRender:\n",
    "        print(f'Epoch [{epoch+1}/{N}],  Log_Likehood: {log_likehood} L2: {loss_val} rL2: {model_vmf.rL2} used: {used}/{all} variance: {variance_helper.get_variance()}')\n",
    "    isFirstWas = True\n",
    "\n",
    "    if onlyRender:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global gt_render_cache\n",
    "gt_render_cache = None\n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import numpy as np\n",
    "def scientific_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return '0'\n",
    "    elif x < 0:\n",
    "        return f'{x:.1f}'\n",
    "    else:\n",
    "        exponent = int(np.floor(np.log10(abs(x))))\n",
    "        mantissa = x / (10**exponent)\n",
    "        return f'{mantissa:.1f}e{exponent}'\n",
    "    \n",
    "\n",
    "class ExperimentPreview:\n",
    "    num_samples: int\n",
    "    scene_cfg: SceneConfig\n",
    "\n",
    "    showNIRC: bool\n",
    "    showSH: bool\n",
    "    showVMF: bool\n",
    "\n",
    "    def __init__(self, scene_cfg: SceneConfig, num_samples: int = 64, showNIRC: bool = True, showSH: bool = True, showVMF: bool = True, ncv = None, showNIRCEQMem: bool = True, exposure: float = 1.0):\n",
    "        self.num_samples = num_samples\n",
    "        self.scene_cfg = scene_cfg\n",
    "        self.showNIRC = showNIRC\n",
    "        self.showNIRCEQMem = showNIRCEQMem\n",
    "        self.showSH = showSH\n",
    "        self.exposure = exposure\n",
    "        self.showVMF = showVMF\n",
    "        self.ncv = ncv\n",
    "        \n",
    "\n",
    "    def do(self, gt_render = None, exposures=[5, 5, 5, 5]):\n",
    "        global gt_render_cache\n",
    "\n",
    "        # Set the path to the font file\n",
    "        fpath = \"LinLibertine_R.ttf\"\n",
    "        prop = fm.FontProperties(fname=fpath)\n",
    "        from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "\n",
    "\n",
    "        figsize = (12,6)\n",
    "        plt.figure(figsize=figsize)  # Adjusted for three subplots\n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor('#e6f2ff')\n",
    "        # Plot Bias vs rVar\n",
    "\n",
    "\n",
    "        sci_formatter = FuncFormatter(scientific_formatter)\n",
    "        sci_formatter2 = ScalarFormatter(useMathText=True)\n",
    "        sci_formatter2.set_scientific(True)\n",
    "        sci_formatter2.set_powerlimits((0, 0))\n",
    "        font_scalar = 200\n",
    "        linewidth = 3\n",
    "        markersize = 3\n",
    "\n",
    "        if self.ncv and self.showNIRC:\n",
    "            plt.rcParams['font.size'] = 28  # Change this to a reasonable size\n",
    "            # Unpack epochs and variances\n",
    "            #epochs_ncv_in, variance_values_ncv_in = zip(*ncv_integral_model.variance_helper.get_cached_variance_per_epoch())\n",
    "            epochs_ncv, variance_values_ncv = zip(*self.ncv.variance_helper.get_cached_variance_per_epoch())\n",
    "            epochs_nirc, variance_values_nirc = zip(*nirc_model.variance_helper.get_cached_variance_per_epoch())\n",
    "            #plt.plot(epochs_ncv_in, variance_values_ncv_in, label='NCVin', color=\"#2b5c6f\", marker=\"o\", markersize=markersize, linewidth=linewidth)\n",
    "            plt.plot(epochs_ncv, variance_values_ncv, label='NCV', color=\"#9b5c6f\", marker=\"o\", markersize=markersize, linewidth=linewidth)\n",
    "            plt.plot(epochs_nirc, variance_values_nirc, label='NIRC', color=\"#537b8d\", marker=\"o\", markersize=markersize, linewidth=linewidth)\n",
    "            #plt.gca().xaxis.set_major_formatter(sci_formatter)  # Apply scientific formatting to x-axis\n",
    "            plt.xticks(fontsize=12, fontproperties=prop)\n",
    "            plt.yticks(fontsize=12, fontproperties=prop)\n",
    "                # Plot NCV and NIRC variance data\n",
    "\n",
    "            #plt.gca().xaxis.set_major_formatter(sci_formatter)\n",
    "            plt.gca().yaxis.set_major_formatter(sci_formatter)  # Apply scientific formatting to x-axis\n",
    "            #plt.gca().xaxis.set_major_locator(MaxNLocator(nbins=3))  # Limit the number of ticks on X axis to 4\n",
    "            plt.gca().yaxis.set_major_locator(MaxNLocator(nbins=3))  # Limit the number of ticks on Y axis to 3\n",
    "            plt.grid(True, linestyle='-', alpha=1.0, color=\"white\", linewidth=2)\n",
    "            plt.gca().set_facecolor('#e6f2ff')\n",
    "            plt.legend(loc='upper right', fontsize=22, prop=prop)\n",
    "\n",
    "            h_pad = 0.0\n",
    "            w_pad = 0.0\n",
    "            pad = 0.0\n",
    "            plt.tight_layout(h_pad=h_pad, w_pad=w_pad,pad=pad)\n",
    "            plt.savefig(os.path.join(scene_cfg.checkpoint_dir, f'variance.svg'), format='svg')\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        if gt_render == None:\n",
    "            if gt_render_cache is None:\n",
    "                self.scene_cfg.prepare_scene(ref_scene)\n",
    "                gt_render_cache = frameRender(num_samples=1, tonemapped=True, vis=False)\n",
    "            \n",
    "            gt_render = gt_render_cache\n",
    "\n",
    "        \n",
    "        fig, ax = plt.subplots(1)\n",
    "        pi_colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'black', 'white']\n",
    "\n",
    "        ax.imshow(gt_render)\n",
    "\n",
    "        for i, p in enumerate(scene_cfg.validation_points):\n",
    "            color = pi_colors[i % len(pi_colors)]\n",
    "            circle = patches.Circle((p.x, p.y), gparams.render_width // 100, facecolor=color, edgecolor='black', linewidth=(gparams.render_width // 500))\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        plt.show()\n",
    "        plt.savefig(os.path.join(scene_cfg.checkpoint_dir, f'experiment_preview.png'))\n",
    "\n",
    "        n_cameras = len(scene_cfg.validation_points)\n",
    "        figure, axes = plt.subplots(n_cameras, 6, figsize=(25, 6 * n_cameras))  # Adding another column for SH render\n",
    "        figure.suptitle('Ground Truth, Predicted Renders, and SH Renders', fontsize=16)\n",
    "\n",
    "        if n_cameras == 1:\n",
    "            axes = np.array([axes])\n",
    "        \n",
    "        for i, c in enumerate(scene_cfg.validation_points):\n",
    "            scene_cfg.prepare_scene(ref_scene, debugPointID=i)\n",
    "            validation_gt_render = frameRender(num_samples=self.num_samples, vis=False, tonemapped=False, directEmissive=False, directSky=True)\n",
    "            brdf = getBRDF([scene_cfg.validation_points[i].x, scene_cfg.validation_points[i].y])\n",
    "            brdf_gpu = torch.from_numpy(brdf).cuda()\n",
    "            brdf_gpu = brdf_gpu.view(-1, 3)\n",
    "            \n",
    "            \n",
    "            validation_gt_render = (validation_gt_render*brdf)\n",
    "            #validation_gt_render = validation_gt_render[:, :, 0]\n",
    "\n",
    "            # Extract pixel-specific data\n",
    "            pi = c.id\n",
    "            # Generate rays and other inputs for the model\n",
    "            mlDataRaysOutput = falcor_to_torch_split_interleaved(ml_rays_data, rays_fields)\n",
    "            positions = mlDataRaysOutput[\"worldpos\"]+0.0\n",
    "            directions = mlDataRaysOutput[\"dir\"]+0.0\n",
    "\n",
    "            color = mlDataOutput_ref[\"color\"][pi]+0.0\n",
    "            r = mlDataOutput_ref[\"roughness\"][pi]+0.0\n",
    "            n = mlDataOutput_ref[\"normal\"][pi]+0.0\n",
    "            v = mlDataOutput_ref[\"view\"][pi]+0.0\n",
    "\n",
    "            exposure = exposures[i]\n",
    "\n",
    "\n",
    "\n",
    "            if True:\n",
    "                tonemapped_gt = apply_gamma_correction(aces_tonemap(validation_gt_render, exposure=exposure))\n",
    "                # Visualization\n",
    "                axes[i, 0].imshow(tonemapped_gt)\n",
    "                axes[i, 0].set_title('Ground Truth', fontsize=10)\n",
    "                plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_gt{i}.png'), tonemapped_gt)\n",
    "                if False:\n",
    "                    padding = gparams.render_width // 100\n",
    "                    \n",
    "                    padded_image = np.pad(tonemapped_gt, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "                    \n",
    "                    padded_image[:padding, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                    padded_image[-padding:, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                    padded_image[:, :padding, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                    padded_image[:, -padding:, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                    plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_gt{i}.png'), tonemapped_gt)\n",
    "                        \n",
    "\n",
    "            if self.showNIRC:\n",
    "                roughness = r[None, :] * torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "                colors_arr = color[None, :] * torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "                normals = n[None, :] * torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "\n",
    "\n",
    "                X = nirc_model.prepare_input_(positions, directions, colors_arr, roughness, normals)\n",
    "                assert X.shape[1] == 12, 'Input shape mismatch'\n",
    "                \n",
    "                Y = nirc_model(X, thp=brdf_gpu).detach()\n",
    "                Y_image = Y.view(gparams.render_height, gparams.render_width, 3).detach().cpu().numpy()\n",
    "                Y_image = Y_image\n",
    "                Y_image = Y_image.astype(np.float32)\n",
    "                \n",
    "                l2_error_model = np.linalg.norm(validation_gt_render - Y_image) / np.sqrt(validation_gt_render.size)\n",
    "                tonemapped_nirc = apply_gamma_correction(aces_tonemap(Y_image, exposure=exposure))\n",
    "                \n",
    "                # Adding colored padding\n",
    "                padding = gparams.render_width // 100\n",
    "                padded_image = np.pad(tonemapped_nirc, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "                \n",
    "                padded_image[:padding, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[-padding:, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, :padding, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, -padding:, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                \n",
    "                plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_nirc{i}.png'), tonemapped_nirc)\n",
    "\n",
    "                axes[i, 1].imshow(padded_image)\n",
    "                axes[i, 1].set_title(f'NIRC Render rL2 = {nirc_model.rL2}:.4f rVar = {nirc_model.variance_helper.get_variance():.4f}', fontsize=10)\n",
    "                \n",
    "            if self.showNIRCEQMem:\n",
    "                \n",
    "                pixel_indices = torch.ones((positions.shape[0]), device=positions.device) * pi  \n",
    "                pixel_indices = torch.stack((pixel_indices % gparams.render_width, pixel_indices // gparams.render_width), dim=1).to(torch.int) \n",
    "            \n",
    "                dirs, pixel_indices = nirc_eqmem_model.prepare_input_(directions, pixel_indices)\n",
    "                Y = nirc_eqmem_model(dirs, pixel_indices=pixel_indices, thp=brdf_gpu).detach()\n",
    "                Y_image = Y.view(gparams.render_height, gparams.render_width, 3).detach().cpu().numpy()\n",
    "                Y_image = Y_image.astype(np.float32)\n",
    "                l2_error_model = np.linalg.norm(validation_gt_render - Y_image) / np.sqrt(validation_gt_render.size)\n",
    "                tonemapped_nirc_eqmem = apply_gamma_correction(aces_tonemap(Y_image, exposure=exposure))\n",
    "                \n",
    "                # Adding colored padding\n",
    "                padding = gparams.render_width // 100\n",
    "                padded_image = np.pad(tonemapped_nirc_eqmem, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "                padded_image[:padding, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[-padding:, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, :padding, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, -padding:, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                \n",
    "                plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_nirc_eqmem{i}.png'), tonemapped_nirc_eqmem)\n",
    "                \n",
    "                axes[i, 2].imshow(padded_image)\n",
    "                axes[i, 2].set_title(f'NIRC PerPixel Render L2 error = {nirc_eqmem_model.rL2:.4f}, var = {nirc_eqmem_model.variance_helper.get_variance():.4f}', fontsize=10)\n",
    "            directions = mlDataRaysOutput[\"dir\"]+0.0    \n",
    "            if self.showSH:\n",
    "                # SH Rendering\n",
    "                shs_render = sh_model(directions, debugID=pi)\n",
    "\n",
    "                shs_render_image = shs_render.view(gparams.render_height, gparams.render_width, 3).detach().cpu().numpy()\n",
    "                shs_render_image = shs_render_image.astype(np.float32)\n",
    "                l2_error_sh = np.linalg.norm(validation_gt_render - shs_render_image)/ np.sqrt(validation_gt_render.size)\n",
    "                tonemapped_shs = apply_gamma_correction(aces_tonemap(shs_render_image, exposure=exposure))\n",
    "                \n",
    "                # Adding colored padding\n",
    "                padding = gparams.render_width // 100\n",
    "                padded_image = np.pad(tonemapped_shs, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "                padded_image[:padding, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[-padding:, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, :padding, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, -padding:, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "\n",
    "                plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_sh{i}.png'), tonemapped_shs)\n",
    "\n",
    "                axes[i, 4].imshow(padded_image)\n",
    "                axes[i, 4].set_title(f'SH Render L2 error = {l2_error_sh:.4f}, rL2 = {sh_model.rL2:.4f}', fontsize=10)\n",
    "\n",
    "            if self.showVMF:\n",
    "                vmf_render = model_vmf(directions, debugID=pi)  # Adjust based on your vmf_coefficients structure\n",
    "                vmf_render_image = vmf_render.view(gparams.render_height, gparams.render_width, 3).detach().cpu().numpy()\n",
    "                vmf_render_image = vmf_render_image.astype(np.float32)\n",
    "                l2_error_vmf = np.linalg.norm(validation_gt_render - vmf_render_image)/ np.sqrt(validation_gt_render.size)\n",
    "                tonemapped_vmf = apply_gamma_correction(aces_tonemap(vmf_render_image, exposure=exposure))\n",
    "                \n",
    "                # Adding colored padding\n",
    "                padding = gparams.render_width // 100\n",
    "                padded_image = np.pad(tonemapped_vmf, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "                padded_image[:padding, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[-padding:, :, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, :padding, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "                padded_image[:, -padding:, :] = colors.to_rgb(pi_colors[i % len(pi_colors)])\n",
    "\n",
    "                plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_vmf{i}.png'), tonemapped_vmf)\n",
    "\n",
    "                axes[i, 5].imshow(padded_image)\n",
    "                axes[i, 5].set_title(f'vMF Render L2 error = {l2_error_vmf:.4f}, rL2 = {model_vmf.rL2:.4f}', fontsize=10)\n",
    "\n",
    "            if self.ncv is not None:\n",
    "                roughness = r[None, :] * torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "                colors_arr = color[None, :] * torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "                normals = n[None, :] * torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "                views = v[None, :]* torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "\n",
    "                pixel_indices = torch.ones((positions.shape[0]), device=positions.device) * pi  \n",
    "                pixel_indices = torch.stack((pixel_indices % gparams.render_width, pixel_indices // gparams.render_width), dim=1).to(torch.int) \n",
    "\n",
    "                c_multiplier = ref_estimations[pi]* torch.ones((positions.shape[0], 1), device=positions.device)\n",
    "                x, jacobian, pixels = self.ncv.prepare_input_(directions, pixel_indices)\n",
    "                world = self.ncv.world_prepare_input_(positions, colors_arr, roughness, normals, views)\n",
    "\n",
    "                Y, integral = self.ncv(x=x, jacobian=jacobian, pixels=pixels, world_data = world, integral_values=c_multiplier, surface_color=colors_arr, detach=True)\n",
    "                \n",
    "\n",
    "                Y_image = Y.view(gparams.render_height, gparams.render_width, 3).detach().cpu().numpy()\n",
    "                Y_image = Y_image.astype(np.float32)\n",
    "                l2_error_model = np.linalg.norm(validation_gt_render - Y_image) / np.sqrt(validation_gt_render.size)\n",
    "                tonemapped_nirc = apply_gamma_correction(aces_tonemap(Y_image, exposure=exposure))\n",
    "                plt.imsave(os.path.join(scene_cfg.checkpoint_dir, f'experiment_ncv{i}.png'), tonemapped_nirc)\n",
    "\n",
    "                axes[i, 3].imshow(tonemapped_nirc)\n",
    "                axes[i,3].set_title(f'NCV Render rL2 = {self.ncv.rL2:.4f} rVar = {self.ncv.variance_helper.get_variance():.4f}', fontsize=10)\n",
    "            \n",
    "    \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(os.path.join(scene_cfg.checkpoint_dir, f'experiment_full_result.png'))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "ex = ExperimentPreview(scene_cfg=scene_cfg, num_samples=128, ncv=ncv_model, showNIRC=True, showNIRCEQMem=True, showSH=False, showVMF=True)\n",
    "ex.do(exposures=[2, 2, 2, 2, 0.1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nirc_model.variance_helper.get_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex = ExperimentPreview(scene_cfg=scene_cfg, showNCV=True ,showNIRC=False, showNIRCEQMem=False, showSH=False, showVMF=False)\n",
    "ex.do(exposures=[3, 3, 1, 4, 0.1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
